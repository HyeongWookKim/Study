{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST MLP - Dropout + Sigmoid (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:27:16.796678Z",
     "start_time": "2021-01-19T11:27:14.564323Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn # 인공 신경망 모델을 설계할 때 필요한 함수를 모아 놓은 모듈\n",
    "import torch.nn.functional as F # torch.nn 모듈 중에서도 자주 이용되는 함수를 'F'로 지정\n",
    "import torch.nn.init as init # Weight, Bias 등 딥러닝 모델에서 초깃값으로 설정되는 요소에 대한 모듈\n",
    "from torchvision import transforms, datasets # 컴퓨터 비전 연구 분야에서 자주 이용하는 torchvision 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:27:16.812349Z",
     "start_time": "2021-01-19T11:27:16.798589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.7.1  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:27:16.858254Z",
     "start_time": "2021-01-19T11:27:16.815201Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 지정\n",
    "BATCH_SIZE = 32  # 미니배치 1개 단위에 대해 데이터가 32개로 구성되어 있는 것을 의미\n",
    "EPOCHS = 10      # 전체 데이터 셋을 10번 반복해 학습한다는 것을 의미 (즉, 전체 데이터를 이용해 학습을 진행한 횟수)\n",
    "\n",
    "# MNIST 데이터 셋 다운로드\n",
    "train_dataset = datasets.MNIST(root = '../data/MNIST',  # 데이터 셋이 저장될 장소를 지정\n",
    "                               train = True,            # 학습용 데이터 셋으로 지정 \n",
    "                               download = True,         # 인터넷 상에서 다운로드해서 이용할 것인지 여부\n",
    "                               transform = transforms.ToTensor()) # 데이터 셋을 tensor 형태로 변경\n",
    "test_dataset = datasets.MNIST(root = '../data/MNIST',\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "# 다운로드한 MNIST 데이터 셋을 미니배치 단위로 분리\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = BATCH_SIZE, # 미니배치 1개 단위를 구성하는 데이터의 개수\n",
    "                                           shuffle = True) # 데이터의 순서를 섞고자 할 때 이용(즉, 잘못된 방향으로 학습하는 것을 방지)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:27:16.888740Z",
     "start_time": "2021-01-19T11:27:16.860703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인하기 (1)\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:27:16.903736Z",
     "start_time": "2021-01-19T11:27:16.890684Z"
    }
   },
   "outputs": [],
   "source": [
    "### 아래의 코드를 실행하면 Kernel이 꺼진다..왜 이러지?\n",
    "# pltsize = 1\n",
    "# plt.figure(figsize = (10 * pltsize, pltsize))\n",
    "# for i in range(10):\n",
    "#     plt.subplot(1, 10, i + 1)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
    "#     plt.title('Class: ' + str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<참고>\n",
    "- ```training = self.training```\n",
    "    - Dropout은 학습 과정 속에서 랜덤으로 노드를 선택해 가중값이 업데이트되지 않도록 조정하지만, 평가 과정 속에서는 모든 노드를 이용하여 Output을 계산하기 때문에 학습 상태와 검증 상태에서 다르게 적용되어야 한다.\n",
    "    - 이를 반영하기 위해 다음과 같이 파라미터를 다르게 설정해줘야 한다.\n",
    "        1. 파라미터 값을 ```model.train()```으로 명시할 때는 ```self.training = True```로 적용\n",
    "        2. 파라미터 값을 ```model.eval()```으로 명시할 때는 ```self.training = False```로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:27:16.918836Z",
     "start_time": "2021-01-19T11:27:16.906244Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multi Layer Perceptron (MLP) 모델 설계하기\n",
    "# PyTorch 모듈 내에 딥러닝 모델 관련 기본 함수를 포함하고 있는 nn.Module 클래스를 상속받는 Net 클래스를 정의\n",
    "class Net(nn.Module):\n",
    "    def __init__(self): # Net 클래스의 인스턴스를 생성했을 때 지니게 되는 성질을 정의\n",
    "        super(Net, self).__init__()        # nn.Module 내에 있는 메서드를 상속받아 사용\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) # 첫 번째 Fully Connected Layer 정의\n",
    "        self.fc2 = nn.Linear(512, 256)     # 두 번째 Fully Connected Layer 정의\n",
    "        self.fc3 = nn.Linear(256, 10)      # 세 번째 Fully Connected Layer 정의\n",
    "        self.dropout_prob = 0.5            # 50%의 노드에 대해 가중값을 계산하지 않도록 설정\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # view 메서드를 통해 2차원 데이터를 784 크기의 1차원 데이터로 변환\n",
    "        \n",
    "        x = self.fc1(x)          # __init__()을 통해 정의한 첫 번째 Fully Connected Layer에 1차원으로 펼친 이미지 데이터를 통과\n",
    "        x = F.sigmoid(x)         # 비선형 함수인 sigmoid()를 이용하여 두 번째 Fully Connected Layer의 Input으로 계산\n",
    "        # sigmoid() 함수의 결과값에 Dropout을 적용\n",
    "        x = F.dropout(x, \n",
    "                      training = self.training,  # 학습 상태일 떄와 검증 상태에 따라 다르게 적용시키기 위한 파라미터\n",
    "                      p = self.dropout_prob)     # 몇 %의 노드에 대해 계산하지 않을 것인지를 설정\n",
    "        \n",
    "        x = self.fc2(x)          # __init__()을 통해 정의한 두 번째 Fully Connected Layer에 앞서 계산된 x를 통과\n",
    "        x = F.sigmoid(x)         # 비선형 함수인 sigmoid()를 이용하여 세 번째 Fully Connected Layer의 Input으로 계산\n",
    "        # sigmoid() 함수의 결과값에 Dropout을 적용\n",
    "        x = F.dropout(x, \n",
    "                      training = self.training,  # 학습 상태일 떄와 검증 상태에 따라 다르게 적용시키기 위한 파라미터\n",
    "                      p = self.dropout_prob)     # 몇 %의 노드에 대해 계산하지 않을 것인지를 설정\n",
    "        \n",
    "        x = self.fc3(x)                # __init__()을 통해 정의한 세 번째 Fully Connected Layer에 앞서 계산된 x를 통과\n",
    "        x = F.log_softmax(x, dim = 1)  # log_softmax()를 이용하여 최종 Output을 계산\n",
    "        # <참고>\n",
    "        # softmax가 아닌 log_softmax를 사용하는 이유는 MLP 모델이 역전파 알고리즘을 이용해 학습을 진행할 때,\n",
    "        # Loss 값에 대한 Gradient 값을 좀 더 원활하게 계산할 수 있기 때문!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:27:16.933987Z",
     "start_time": "2021-01-19T11:27:16.922486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Optimizer, Objective Function 설정하기\n",
    "model = Net().to(DEVICE) # 'DEVICE' 장비를 이용하여 MLP 모델을 완성하기 위해, 앞서 정의한 MLP 모델을 기존에 선정한 'DEVICE'에 할당\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5) # momentum은 Optimizer의 관성을 의미\n",
    "criterion = nn.CrossEntropyLoss() # MLP 모델의 Output 값과 원-핫 인코딩 값인 Label 값의 Loss를 CrossEntropy를 이용하여 계산\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:27:16.948819Z",
     "start_time": "2021-01-19T11:27:16.938644Z"
    }
   },
   "outputs": [],
   "source": [
    "# MLP 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 (MLP 모델을 학습)\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train() # 기존에 정의한 MLP 모델을 '학습' 상태로 지정\n",
    "    # train_loader 내에 미니배치 단위로 저장된 데이터를 순서대로 이용하여 MLP 모델을 학습\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE) # 미니배치 내에 있는 image 데이터를 이용하여 MLP 모델을 학습시키기 위해 기존에 정의한 장비에 할당\n",
    "        label = label.to(DEVICE) # 미니배치 내에 있는 image 데이터와 매칭된 label 데이터도 기존에 정의한 장비에 할당\n",
    "        \n",
    "        optimizer.zero_grad()    # opimizer의 Gradient를 초기화\n",
    "        \n",
    "        output = model(image)    # 장비에 할당한 이미지 데이터를 MLP 모델의 Input으로 이용\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        loss.backward()   # Loss 값을 계산한 결과를 바탕으로 역전파를 통해 계산된 Gradient 값을 각 파라미터에 할당\n",
    "        optimizer.step()  # 각 파라미터에 할당된 Gradient 값을 이용하여 파라미터 값을 업데이트\n",
    "        \n",
    "        # 아래의 코드는 위 함수가 실행되는 과정을 모니터링하기 위한 코드\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), len(train_loader.dataset), \n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:27:16.963918Z",
     "start_time": "2021-01-19T11:27:16.952608Z"
    }
   },
   "outputs": [],
   "source": [
    "# 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 (학습의 진행 과정을 모니터링)\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()   # 학습 과정 또는 학습이 완료된 MLP 모델을 학습 상태가 아닌 '평가' 상태로 지정\n",
    "    test_loss = 0  # 기존에 정의한 test_loader 내의 데이터를 이용하여 Loss 값을 계산하기 위해 '0'으로 임시 설정\n",
    "    correct = 0    # 학습 과정 또는 학습이 완료된 MLP 모델이 올바른 Class로 분류한 경우를 count 하기 위해 '0'으로 임시 설정\n",
    "    \n",
    "    # MLP 모델을 평가하는 단계에서는 Gradient를 통해 파라미터 값이 업데이트 되는 현상을 방지하기 위해,\n",
    "    # torch.no_grad() 메서드를 통해 Gradient의 흐름을 억제\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)  # 미니배치 내에 있는 image 데이터를 이용하여 MLP 모델을 검증하기 위해 기존에 정의한 장비에 할당\n",
    "            label = label.to(DEVICE)  # 미니배치 내에 있는 image 데이터와 매칭된 label 데이터도 기존에 정의한 장비에 할당\n",
    "            \n",
    "            output = model(image)     # 장비에 할당한 image 데이터를 MLP 모델의 Input으로 사용\n",
    "            test_loss += criterion(output, label).item()   # test_loss 값 업데이트\n",
    "            \n",
    "            prediction = output.max(1, keepdim = True)[1]  # 가장 큰 벡터 값의 위치에 대응하는 클래스로 예측했다고 판단\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item() # 최종 예측이 올바른지 count\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset) # 현재까지 계산된 test_loss 값을 미니배치 개수만큼 나눠서 평균 Loss 값을 계산\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset) # 정확도 계산\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:30:25.134428Z",
     "start_time": "2021-01-19T11:27:16.965436Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.467475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brian\\anaconda3\\envs\\DL_torch_cpu\\lib\\site-packages\\torch\\nn\\functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 2.271861\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 2.291526\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 2.400225\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 2.359376\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 2.319067\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 2.339031\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 2.341093\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 2.274296\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 2.297570\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0714, \tTest Accuracy: 10.79 %\n",
      "]\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 2.315763\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 2.276724\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 2.307859\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 2.271694\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 2.276572\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 2.239483\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 2.234201\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 2.106981\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 2.059643\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 2.007169\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0634, \tTest Accuracy: 47.42 %\n",
      "]\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 2.060012\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 2.085054\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 1.900310\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 1.906358\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 1.666402\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 1.687145\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 1.761589\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 1.441975\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 1.439413\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 1.455436\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0379, \tTest Accuracy: 58.53 %\n",
      "]\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 1.433188\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 1.225842\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 1.469291\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 1.534180\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 1.484892\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.993882\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 1.192556\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 1.089712\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 1.222420\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 1.099887\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0277, \tTest Accuracy: 70.96 %\n",
      "]\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 1.122309\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 1.287141\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.843394\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.723787\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 1.151898\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.710054\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.779362\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.876737\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 1.349453\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.949337\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0236, \tTest Accuracy: 76.56 %\n",
      "]\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 1.023463\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.862518\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.905427\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.768327\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.514263\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.680682\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.474998\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.727396\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.840847\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.656225\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0208, \tTest Accuracy: 79.69 %\n",
      "]\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.865991\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.804311\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.780073\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.760060\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 1.092912\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.854787\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.854016\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.458570\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 1.125670\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.865376\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0184, \tTest Accuracy: 82.34 %\n",
      "]\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.629262\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.632561\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.945152\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.932014\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.801247\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.822787\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.671002\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.725142\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.680571\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.724285\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0164, \tTest Accuracy: 84.66 %\n",
      "]\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.863223\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 1.049612\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 1.142056\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.570025\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.831793\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.404556\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.837571\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.898240\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.467845\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.455044\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0151, \tTest Accuracy: 85.57 %\n",
      "]\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.711146\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.487654\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.423471\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.272875\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.587238\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.720471\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.397723\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.540195\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.507374\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.632167\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0141, \tTest Accuracy: 86.69 %\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# MLP 모델 학습을 실행하며 Train/Test set의 Loss 및 Test set Accuracy 확인하기\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200) # MLP 모델 학습 with SGD Optimizer\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)   # 각 epoch별로 출력되는 Loss 값과 accuracy 값을 계산\n",
    "    print('\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n]'.format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST MLP - Dropout + ReLU\n",
    "- ReLU 함수는 0 미만인 값은 0으로 계산하고, 양수 값은 그대로 반영하는 비선형 함수이다.\n",
    "- Dropout은 보통 비선형 함수인 ReLu 함수와 잘 어울린다.\n",
    "    - Sigmoid 함수는 0에서 멀어질수록 Gradient 값이 0에 가까워지므로, Back Propagation이 효과적으로 이용되기 어렵다.\n",
    "    - 반면에, ReLU 함수는 **Gradient Vanishing 문제를 어느 정도 해결**해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<참고>\n",
    "- \"Multi Layer Perceptron (MLP) 모델 설계\" 부분을 제외한 나머지 과정은 앞과 동일하므로 생략하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:30:25.150855Z",
     "start_time": "2021-01-19T11:30:25.135953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multi Layer Perceptron (MLP) 모델 설계하기\n",
    "# PyTorch 모듈 내에 딥러닝 모델 관련 기본 함수를 포함하고 있는 nn.Module 클래스를 상속받는 Net 클래스를 정의\n",
    "class Net(nn.Module):\n",
    "    def __init__(self): # Net 클래스의 인스턴스를 생성했을 때 지니게 되는 성질을 정의\n",
    "        super(Net, self).__init__()        # nn.Module 내에 있는 메서드를 상속받아 사용\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) # 첫 번째 Fully Connected Layer 정의\n",
    "        self.fc2 = nn.Linear(512, 256)     # 두 번째 Fully Connected Layer 정의\n",
    "        self.fc3 = nn.Linear(256, 10)      # 세 번째 Fully Connected Layer 정의\n",
    "        self.dropout_prob = 0.5            # 50%의 노드에 대해 가중값을 계산하지 않도록 설정\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # view 메서드를 통해 2차원 데이터를 784 크기의 1차원 데이터로 변환\n",
    "        \n",
    "        x = self.fc1(x)          # __init__()을 통해 정의한 첫 번째 Fully Connected Layer에 1차원으로 펼친 이미지 데이터를 통과\n",
    "        x = F.relu(x)            # 비선형 함수인 relu()를 이용하여 두 번째 Fully Connected Layer의 Input으로 계산\n",
    "        # sigmoid() 함수의 결과값에 Dropout을 적용\n",
    "        x = F.dropout(x, \n",
    "                      training = self.training,  # 학습 상태일 떄와 검증 상태에 따라 다르게 적용시키기 위한 파라미터\n",
    "                      p = self.dropout_prob)     # 몇 %의 노드에 대해 계산하지 않을 것인지를 설정\n",
    "        \n",
    "        x = self.fc2(x)          # __init__()을 통해 정의한 두 번째 Fully Connected Layer에 앞서 계산된 x를 통과\n",
    "        x = F.relu(x)            # 비선형 함수인 relu()를 이용하여 세 번째 Fully Connected Layer의 Input으로 계산\n",
    "        # sigmoid() 함수의 결과값에 Dropout을 적용\n",
    "        x = F.dropout(x, \n",
    "                      training = self.training,  # 학습 상태일 떄와 검증 상태에 따라 다르게 적용시키기 위한 파라미터\n",
    "                      p = self.dropout_prob)     # 몇 %의 노드에 대해 계산하지 않을 것인지를 설정\n",
    "        \n",
    "        x = self.fc3(x)                # __init__()을 통해 정의한 세 번째 Fully Connected Layer에 앞서 계산된 x를 통과\n",
    "        x = F.log_softmax(x, dim = 1)  # log_softmax()를 이용하여 최종 Output을 계산\n",
    "        # <참고>\n",
    "        # softmax가 아닌 log_softmax를 사용하는 이유는 MLP 모델이 역전파 알고리즘을 이용해 학습을 진행할 때,\n",
    "        # Loss 값에 대한 Gradient 값을 좀 더 원활하게 계산할 수 있기 때문!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:30:25.181680Z",
     "start_time": "2021-01-19T11:30:25.154666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Optimizer, Objective Function 설정하기\n",
    "model = Net().to(DEVICE) # 'DEVICE' 장비를 이용하여 MLP 모델을 완성하기 위해, 앞서 정의한 MLP 모델을 기존에 선정한 'DEVICE'에 할당\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5) # momentum은 Optimizer의 관성을 의미\n",
    "criterion = nn.CrossEntropyLoss() # MLP 모델의 Output 값과 원-핫 인코딩 값인 Label 값의 Loss를 CrossEntropy를 이용하여 계산\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:33:32.285024Z",
     "start_time": "2021-01-19T11:30:25.184202Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.318754\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 2.104750\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 1.228628\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.843933\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.691317\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.748156\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.724201\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.505370\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.541805\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.270850\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0100, \tTest Accuracy: 90.95 %\n",
      "]\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.219867\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.301647\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.247123\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.344572\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.370230\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.361282\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.136813\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.192550\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.163311\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.260349\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0072, \tTest Accuracy: 93.36 %\n",
      "]\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.223250\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.605482\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.528076\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.146051\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.167137\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.284976\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.081580\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.145251\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.224026\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.130921\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0054, \tTest Accuracy: 94.89 %\n",
      "]\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.087477\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.172936\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.508992\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.095676\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.296446\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.418514\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.239304\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.167711\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.285959\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.197518\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0044, \tTest Accuracy: 95.61 %\n",
      "]\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.296645\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.078141\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.132212\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.110109\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.367680\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.108822\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.126249\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.291046\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.344990\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.317976\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0039, \tTest Accuracy: 96.08 %\n",
      "]\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.067737\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.292100\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.091256\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.318147\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.234266\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.109612\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.313058\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.205198\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.165711\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.099755\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0035, \tTest Accuracy: 96.52 %\n",
      "]\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.301382\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.251990\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.396911\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.048268\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.197057\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.106094\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.073560\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.203839\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.150583\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.073552\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0032, \tTest Accuracy: 96.90 %\n",
      "]\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.146754\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.087864\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.115530\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.227674\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.074492\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.059246\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.166972\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.045357\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.090899\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.208551\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0029, \tTest Accuracy: 97.07 %\n",
      "]\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.091408\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.594694\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.064265\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.020487\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.027978\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.160308\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.265301\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.304039\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.115917\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.177544\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0028, \tTest Accuracy: 97.25 %\n",
      "]\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.005549\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.060066\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.413149\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.220173\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.011337\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.068678\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.077964\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.135458\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.042021\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.042691\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0027, \tTest Accuracy: 97.32 %\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# MLP 모델 학습을 실행하며 Train/Test set의 Loss 및 Test set Accuracy 확인하기\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200) # MLP 모델 학습 with SGD Optimizer\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)   # 각 epoch별로 출력되는 Loss 값과 accuracy 값을 계산\n",
    "    print('\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n]'.format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과를 보면, sigmoid() 함수를 사용했을 때보다 ReLU() 함수를 사용했을 때 성능이 좋아진 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST MLP - Dropout + ReLU + Batch Normalization\n",
    "- 신경망에는 과적합과 Gradient Vanishing 외에도 **Internal Covariance shift**라는 현상이 발생한다.\n",
    "- Internal Covariance shift란, 각 Layer마다 Input 분포가 달라짐에 따라 학습 속도가 느려지는 현상을 말한다.\n",
    "- 이를 방지하기 위해, Batch Normalization을 사용한다.\n",
    "    - Layer의 Input 분포를 정규화해서 학습 속도를 빠르게 하겠다는 것! (즉, 정규화를 통해 비선형 활성 함수의 의미를 살리는 개념)\n",
    "    - Batch Normalization을 사용하면 학습 속도를 향상시켜주고, Gradient Vanishing 문제도 완화해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<참고>\n",
    "- \"Multi Layer Perceptron (MLP) 모델 설계\" 부분을 제외한 나머지 과정은 앞과 동일하므로 생략하겠다.\n",
    "- 또한 Batch Normalization은 1차원, 2차원, 3차원 등 다양한 차원에 따라 적용되는 함수명이 다르기 때문에 유의해서 사용해야 한다.\n",
    "    - MLP 내 각 Layer에서 데이터는 1차원 크기의 벡터 값을 계산하기 때문에 ```nn.BatchNorm1d()```를 이용한다.\n",
    "- ```nn.BatchNorm()``` 함수를 이용해 적용하는 부분은 연구자들의 선호에 따라 다음과 같이 2가지 경우로 나뉜다.\n",
    "    1. Activation Function 이전에 적용 (아래의 예제에서는 이 방법을 사용하였음)\n",
    "    2. Activation Function 이후에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:33:32.302489Z",
     "start_time": "2021-01-19T11:33:32.287555Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multi Layer Perceptron (MLP) 모델 설계하기\n",
    "# PyTorch 모듈 내에 딥러닝 모델 관련 기본 함수를 포함하고 있는 nn.Module 클래스를 상속받는 Net 클래스를 정의\n",
    "class Net(nn.Module):\n",
    "    def __init__(self): # Net 클래스의 인스턴스를 생성했을 때 지니게 되는 성질을 정의\n",
    "        super(Net, self).__init__()        # nn.Module 내에 있는 메서드를 상속받아 사용\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) # 첫 번째 Fully Connected Layer 정의\n",
    "        self.fc2 = nn.Linear(512, 256)     # 두 번째 Fully Connected Layer 정의\n",
    "        self.fc3 = nn.Linear(256, 10)      # 세 번째 Fully Connected Layer 정의\n",
    "        self.dropout_prob = 0.5            # 50%의 노드에 대해 가중값을 계산하지 않도록 설정\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512) # 첫 번째 Fully Connected Layer의 Output이 512 크기의 벡터값 \n",
    "        self.batch_norm2 = nn.BatchNorm1d(256) # 두 번째 Fully Connected Layer의 Output이 256 크기의 벡터값 \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # view 메서드를 통해 2차원 데이터를 784 크기의 1차원 데이터로 변환\n",
    "        \n",
    "        x = self.fc1(x)          # __init__()을 통해 정의한 첫 번째 Fully Connected Layer에 1차원으로 펼친 이미지 데이터를 통과\n",
    "        x = self.batch_norm1(x)  # Activation Function 이전에 Batch Normalization을 적용\n",
    "        x = F.relu(x)            # 비선형 함수인 relu()를 이용하여 두 번째 Fully Connected Layer의 Input으로 계산\n",
    "        # sigmoid() 함수의 결과값에 Dropout을 적용\n",
    "        x = F.dropout(x, \n",
    "                      training = self.training,  # 학습 상태일 떄와 검증 상태에 따라 다르게 적용시키기 위한 파라미터\n",
    "                      p = self.dropout_prob)     # 몇 %의 노드에 대해 계산하지 않을 것인지를 설정\n",
    "        \n",
    "        x = self.fc2(x)          # __init__()을 통해 정의한 두 번째 Fully Connected Layer에 앞서 계산된 x를 통과\n",
    "        x = self.batch_norm2(x)  # Activation Function 이전에 Batch Normalization을 적용\n",
    "        x = F.relu(x)            # 비선형 함수인 relu()를 이용하여 세 번째 Fully Connected Layer의 Input으로 계산\n",
    "        # sigmoid() 함수의 결과값에 Dropout을 적용\n",
    "        x = F.dropout(x, \n",
    "                      training = self.training,  # 학습 상태일 떄와 검증 상태에 따라 다르게 적용시키기 위한 파라미터\n",
    "                      p = self.dropout_prob)     # 몇 %의 노드에 대해 계산하지 않을 것인지를 설정\n",
    "        \n",
    "        x = self.fc3(x)                # __init__()을 통해 정의한 세 번째 Fully Connected Layer에 앞서 계산된 x를 통과\n",
    "        x = F.log_softmax(x, dim = 1)  # log_softmax()를 이용하여 최종 Output을 계산\n",
    "        # <참고>\n",
    "        # softmax가 아닌 log_softmax를 사용하는 이유는 MLP 모델이 역전파 알고리즘을 이용해 학습을 진행할 때,\n",
    "        # Loss 값에 대한 Gradient 값을 좀 더 원활하게 계산할 수 있기 때문!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:33:32.331279Z",
     "start_time": "2021-01-19T11:33:32.305174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Optimizer, Objective Function 설정하기\n",
    "model = Net().to(DEVICE) # 'DEVICE' 장비를 이용하여 MLP 모델을 완성하기 위해, 앞서 정의한 MLP 모델을 기존에 선정한 'DEVICE'에 할당\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5) # momentum은 Optimizer의 관성을 의미\n",
    "criterion = nn.CrossEntropyLoss() # MLP 모델의 Output 값과 원-핫 인코딩 값인 Label 값의 Loss를 CrossEntropy를 이용하여 계산\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:40:20.923231Z",
     "start_time": "2021-01-19T11:33:32.333799Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.631368\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.587032\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.428954\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.562645\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.339285\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.324099\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.242714\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.401987\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.159260\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.243362\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0049, \tTest Accuracy: 95.26 %\n",
      "]\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.192757\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.312221\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.205559\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.181145\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.363126\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.293332\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.192494\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.203621\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.303225\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.136309\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0036, \tTest Accuracy: 96.49 %\n",
      "]\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.222666\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.187882\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.416167\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.326855\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.394058\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.305077\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.203777\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.262950\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.018024\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.108620\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0030, \tTest Accuracy: 97.11 %\n",
      "]\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.328937\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.166699\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.130414\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.038970\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.095110\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.222994\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.065666\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.311348\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.088214\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.050323\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0027, \tTest Accuracy: 97.36 %\n",
      "]\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.213817\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.313095\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.266827\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.067663\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.385210\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.457410\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.103080\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.096533\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.267369\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.217049\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0025, \tTest Accuracy: 97.60 %\n",
      "]\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.310951\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.166813\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.059372\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.352883\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.032866\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.075284\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.154709\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.040918\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.175006\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.566631\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0024, \tTest Accuracy: 97.69 %\n",
      "]\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.058894\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.106047\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.268632\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.231825\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.027372\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.059775\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.067241\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.184124\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.023329\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.120392\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0023, \tTest Accuracy: 97.83 %\n",
      "]\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.247853\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.117173\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.066418\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.057035\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.097464\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.170140\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.059647\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.172168\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.129657\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.048996\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0021, \tTest Accuracy: 97.93 %\n",
      "]\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.241320\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.037084\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.036396\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.080906\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.144642\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.009648\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.245459\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.126971\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.107603\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.055542\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0020, \tTest Accuracy: 97.91 %\n",
      "]\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.057359\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.110312\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.197745\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.078942\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.307721\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.065261\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.091249\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.138066\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.271332\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.029000\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0020, \tTest Accuracy: 98.10 %\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# MLP 모델 학습을 실행하며 Train/Test set의 Loss 및 Test set Accuracy 확인하기\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200) # MLP 모델 학습 with SGD Optimizer\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)   # 각 epoch별로 출력되는 Loss 값과 accuracy 값을 계산\n",
    "    print('\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n]'.format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST MLP - Dropout + ReLU + Batch Normalization + He\n",
    "- 신경망은 처음에 Weight를 랜덤하게 초기화하고 Loss가 최소화되는 부분을 찾아간다.\n",
    "- 따라서 Weight의 초깃값을 어떻게 설정하느냐에 따라 학습 속도가 달라질 수 있다.\n",
    "    - 대표적인 초깃값으로는 LeCun, Xavier, He가 있다.\n",
    "        - ReLU 함수를 사용할 때 비효율적인 Xavier 초깃값을 보완한 것이 He 초깃값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<참고>\n",
    "- \"Optimizer, Objective Function 설정\" 부분을 제외한 나머지 과정은 앞과 동일하므로 생략하겠다.\n",
    "- PyTorch 내의 ```nn.Linear```는 기본값으로 **균등 분포(Uniform Distribution)**에서 샘플링을 통해 파라미터를 초기화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<참고>\n",
    "- ```torch.nn.init``` 모듈 참조 사이트\n",
    "    - https://pytorch.org/docs/stable/nn.init.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:40:20.954637Z",
     "start_time": "2021-01-19T11:40:20.925756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# MLP 모델 내의 Weight를 초기화할 부분을 설정하기 위한 함수\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):              # MLP 모델을 구성하고 있는 파라미터 중, nn.Linear에 해당하는 파라미터 값에 대해서만 지정\n",
    "        init.kaiming_uniform_(m.weight.data)  # nn.Linear에 해당하는 파라미터 값에 대해 He 초깃값을 이용해 파라미터 값을 초기화\n",
    "\n",
    "# Optimizer, Objective Function 설정하기\n",
    "model = Net().to(DEVICE) # 'DEVICE' 장비를 이용하여 MLP 모델을 완성하기 위해, 앞서 정의한 MLP 모델을 기존에 선정한 'DEVICE'에 할당\n",
    "model.apply(weight_init) # weight_init 함수를 Net 클래스의 인스턴스인 model에 적용\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5) # momentum은 Optimizer의 관성을 의미\n",
    "criterion = nn.CrossEntropyLoss() # MLP 모델의 Output 값과 원-핫 인코딩 값인 Label 값의 Loss를 CrossEntropy를 이용하여 계산\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:46:19.791347Z",
     "start_time": "2021-01-19T11:40:20.957150Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 3.170011\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.890471\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.518925\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.906364\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.617774\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.843424\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.843766\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.104850\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.190667\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.247100\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0068, \tTest Accuracy: 93.50 %\n",
      "]\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.671495\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.474878\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.301049\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.161757\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.234469\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.207072\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.319029\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.242849\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.108940\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.242102\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0054, \tTest Accuracy: 94.77 %\n",
      "]\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.280105\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.163313\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.345818\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.060281\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.470300\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.181112\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.280845\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.418553\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.807655\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.171427\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0046, \tTest Accuracy: 95.50 %\n",
      "]\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.405981\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.596009\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.086744\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.231632\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.567335\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.408857\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.428469\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.135925\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.297523\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.323911\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0042, \tTest Accuracy: 95.83 %\n",
      "]\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.323804\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.700629\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.173236\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.217542\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.284017\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.065173\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.210462\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.582436\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.046464\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.226804\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0037, \tTest Accuracy: 96.46 %\n",
      "]\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.525379\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.111106\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.426941\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.283208\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.260846\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.271462\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.346571\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.232744\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.179537\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.075242\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0035, \tTest Accuracy: 96.73 %\n",
      "]\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.395525\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.163040\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.038934\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.071559\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.196789\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.622484\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.084923\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.209830\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.061287\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.128485\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0032, \tTest Accuracy: 96.80 %\n",
      "]\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.076325\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.204815\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.161877\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.195649\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.319516\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.196567\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.171627\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.145598\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.085251\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.099186\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0031, \tTest Accuracy: 96.93 %\n",
      "]\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.362176\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.306857\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.262649\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.129549\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.060234\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.179585\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.075856\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.242555\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.205964\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.472769\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0029, \tTest Accuracy: 97.25 %\n",
      "]\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.136256\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.131754\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.036832\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.225988\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.170603\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.226844\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.234951\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.076494\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.084874\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.079304\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0029, \tTest Accuracy: 97.21 %\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# MLP 모델 학습을 실행하며 Train/Test set의 Loss 및 Test set Accuracy 확인하기\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200) # MLP 모델 학습 with SGD Optimizer\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)   # 각 epoch별로 출력되는 Loss 값과 accuracy 값을 계산\n",
    "    print('\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n]'.format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST MLP - Dropout + ReLU + Batch Normalization + He + Adam\n",
    "- 많이 사용되는 Stochastic Gradient Descent(SGD) 외에도, 다음과 같이 다양한 Optimizer가 존재한다.\n",
    "    1. Momentum\n",
    "        - SGD보다 최적의 장소로 더 빠르게 수렴(즉, 보폭을 크게 하는 개념)한다.\n",
    "        - 최적해가 아닌 지역해를 지나칠 수 있다는 장점이 있다.\n",
    "    2. Nesterov Accelerated Gradient(NAG)\n",
    "        - Momentum을 변형한 방법이다.\n",
    "    3. Adaptive Gradient(Adagrad)\n",
    "        - Adagrad의 개념은 \"가보지 않은 곳은 많이 움직이고, 가본 곳은 조금씩 움직이자\"이다.\n",
    "    4. RMSProp\n",
    "        - Adagrad의 단점을 보완한 방법이다.\n",
    "            - Adagrad는 학습이 오래 진행될수록 부분이 계속 증가해 step size가 작아진다는 문제가 있다.\n",
    "    5. Adaptive Delta(Adadelta)\n",
    "        - Adagrad의 단점을 보완한 방법이다.\n",
    "        - Gradient의 양이 너무 적어지면 움직임이 멈출 수 있는데, 이를 방지하기 위한 방법이다.\n",
    "    6. **Adaptive Moment Estimation(Adam)**\n",
    "        - 딥러닝 모델을 디자인할 때, **기본적으로 가장 많이 사용하는 Optimizer**이다.\n",
    "        - RMSEProp과 Momentum 방식의 특징을 결합한 방법이다.\n",
    "    7. Rectified Adam Optimizer(RAdam)\n",
    "        - 대부분의 Optimizer는 학습 초기에 Bad Local Optimum에 수렴해 버릴 수 있는 단점이 있다.\n",
    "            - 즉, 학습 초기에 Gradient가 매우 작아 져서 학습이 더 이상 일어나지 않는 현상이 발생\n",
    "        - 이러한 Adaptive Learning Rate Term의 분산을 교정(Recify)하는 Optimizer이다.\n",
    "            - Learning Rate를 어떻게 조절하든 성능이 비슷하다.\n",
    "            - 즉, Learning Rate에 민감하지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:46:19.822052Z",
     "start_time": "2021-01-19T11:46:19.794538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# MLP 모델 내의 Weight를 초기화할 부분을 설정하기 위한 함수\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):              # MLP 모델을 구성하고 있는 파라미터 중, nn.Linear에 해당하는 파라미터 값에 대해서만 지정\n",
    "        init.kaiming_uniform_(m.weight.data)  # nn.Linear에 해당하는 파라미터 값에 대해 He 초깃값을 이용해 파라미터 값을 초기화\n",
    "\n",
    "# Optimizer, Objective Function 설정하기\n",
    "model = Net().to(DEVICE) # 'DEVICE' 장비를 이용하여 MLP 모델을 완성하기 위해, 앞서 정의한 MLP 모델을 기존에 선정한 'DEVICE'에 할당\n",
    "model.apply(weight_init) # weight_init 함수를 Net 클래스의 인스턴스인 model에 적용\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = 0.01, momentum = 0.5) # momentum은 Optimizer의 관성을 의미\n",
    "criterion = nn.CrossEntropyLoss() # MLP 모델의 Output 값과 원-핫 인코딩 값인 Label 값의 Loss를 CrossEntropy를 이용하여 계산\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:53:42.097684Z",
     "start_time": "2021-01-19T11:46:19.825609Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.930486\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.225214\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.913058\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.329149\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.204424\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.203714\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.510089\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.242505\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.241195\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.109743\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0041, \tTest Accuracy: 95.66 %\n",
      "]\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.347754\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.185216\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.187265\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.332085\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.342244\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.238569\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.166769\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.039649\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.285283\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.094551\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0033, \tTest Accuracy: 96.53 %\n",
      "]\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.243766\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.389643\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.076345\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.097186\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.247963\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.555981\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.358951\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.406272\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.207483\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.282743\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0030, \tTest Accuracy: 97.02 %\n",
      "]\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.046645\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.075207\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.045210\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.311178\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.073343\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.245552\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.063687\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.302865\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.072111\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.330796\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0026, \tTest Accuracy: 97.37 %\n",
      "]\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.038606\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.152691\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.162162\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.242923\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.174728\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.033206\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.049438\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.304870\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.166936\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.225114\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0026, \tTest Accuracy: 97.39 %\n",
      "]\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.207378\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.131621\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.463968\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.280310\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.324216\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.035280\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.115431\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.179784\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.234262\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.049688\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0025, \tTest Accuracy: 97.63 %\n",
      "]\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.148147\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.026218\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.150484\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.309696\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.217425\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.354001\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.084145\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.534854\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.836176\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.389333\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0024, \tTest Accuracy: 97.68 %\n",
      "]\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.080776\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.242303\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.165626\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.245915\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.268631\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.079257\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.222695\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.078653\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.056633\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.289794\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0023, \tTest Accuracy: 97.64 %\n",
      "]\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.048685\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.101777\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.103143\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.135580\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.685602\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.152000\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.224330\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.060197\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.149601\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.029087\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0020, \tTest Accuracy: 98.19 %\n",
      "]\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.031746\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.157587\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.069359\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.137750\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.069520\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.048069\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.142145\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.101347\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.068359\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.181704\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0022, \tTest Accuracy: 97.91 %\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# MLP 모델 학습을 실행하며 Train/Test set의 Loss 및 Test set Accuracy 확인하기\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200) # MLP 모델 학습 with SGD Optimizer\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)   # 각 epoch별로 출력되는 Loss 값과 accuracy 값을 계산\n",
    "    print('\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n]'.format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FashionMNIST - AutoEncoder\n",
    "1. AutoEncoder(AE)\n",
    "    - 대표적인 **비지도학습 신경망 모델**이다.\n",
    "    - AutoEncoder를 활용하면 Input Data를 Latent Space에 압축시켜, 이 값을 새로운 Feature로 사용할 수 있다.\n",
    "        - 즉, **Feature Extraction의 일종**으로 새로운 Feature를 사용했을 때 기존의 Feature를 사용할 때보다 성능이 좋고, 차원을 줄일 수 있다는 장점이 있다.\n",
    "    - AutoEncoder의 학습 과정은 **데이터를 원래의 데이터로 잘 복원하도록 학습시키는 것**으로 이해할 수 있다.\n",
    "    \n",
    "2. Stacked AutoEncoder(SAE)\n",
    "    - 말 그대로 **AutoEncoder를 쌓아올린 모델**이다.\n",
    "        - 즉, AutoEncoder의 새로운 Feature가 Feature로서의 의미가 있다면, 이를 쌓아 올려서 학습하면 더 좋은 학습 모델을 만들 수 있을 것이라는 생각으로부터 만들어진 모델이다.\n",
    "            - 다시 말해, **\"좋은 Feature를 지니고 있는 Hidden Layer를 쌓아 네트워크를 학습시키면 더 좋은 모델을 만들 수 있을 것이다\"**라는 개념이다.\n",
    "        - 학습 과정은 다음과 같다.\n",
    "            1. Input Data로 AutoEncoder1을 학습\n",
    "            2. 1번에서 학습된 모형의 Hidden Layer를 Input으로 해서 AutoEncoder2를 학습\n",
    "            3. 2번 과정을 원하는 만큼 반복\n",
    "            4. 1 ~ 3번 과정에서 학습된 Hidden Layer를 쌓아 올림\n",
    "            5. 마지막 Layer에 Softmax와 같은 분류 기능이 있는 Output Layer를 추가\n",
    "            6. Fine-tuning으로 전체 다충 신경망을 재학습\n",
    "                - **Pre-trained Model**: 미리 학습시킨 모델\n",
    "                - **Fine-tuning**: 따로 학습시킨 모델을 재학습시키는 개념(즉, Pre-trained Model을 재학습시키는 과정)\n",
    "3. Denoising AutoEncoder(DAE)\n",
    "    - **더 강건한(robust) Feature**를 만들기 위한 AutoEncoder이다.\n",
    "    - 이를 위해, **Input Data에 약간의 Noise를 추가해서 학습**시킨다.\n",
    "        - 즉, 어떤 데이터가 Input으로 와도 강건한 모델을 만들겠다는 의미이다.\n",
    "        \n",
    "    <참고>\n",
    "    - Stacked Denoising AutoEncoder(SDAE)는 Stacked AutoEncoder에서 AutoEncoder를 Denoising AutoEncoder로 대체한 모형이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:53:42.158215Z",
     "start_time": "2021-01-19T11:53:42.099218Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 지정\n",
    "BATCH_SIZE = 32  # 미니배치 1개 단위에 대해 데이터가 32개로 구성되어 있는 것을 의미\n",
    "EPOCHS = 10      # 전체 데이터 셋을 10번 반복해 학습한다는 것을 의미 (즉, 전체 데이터를 이용해 학습을 진행한 횟수)\n",
    "\n",
    "# FashionMNIST 데이터 다운로드\n",
    "train_dataset = datasets.FashionMNIST(root = '../data/FashionMNIST',             # 데이터 셋이 저장될 장소를 지정\n",
    "                                      train = True,                       # 학습용 데이터 셋으로 지정 \n",
    "                                      download = True,                    # 인터넷 상에서 다운로드해서 이용할 것인지 여부\n",
    "                                      transform = transforms.ToTensor())  # 데이터 셋을 tensor 형태로 변경\n",
    "test_dataset = datasets.FashionMNIST(root = '../data/FashionMNIST',\n",
    "                                     train = False,\n",
    "                                     download = True,\n",
    "                                     transform = transforms.ToTensor())\n",
    "\n",
    "# 다운로드한 FashionMNIST 데이터 셋을 미니배치 단위로 분리\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = BATCH_SIZE, # 미니배치 1개 단위를 구성하는 데이터의 개수\n",
    "                                           shuffle = True) # 데이터의 순서를 섞고자 할 때 이용(즉, 잘못된 방향으로 학습하는 것을 방지)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:53:42.189114Z",
     "start_time": "2021-01-19T11:53:42.160733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인하기 (1)\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:53:42.204129Z",
     "start_time": "2021-01-19T11:53:42.195750Z"
    }
   },
   "outputs": [],
   "source": [
    "### 아래의 코드를 실행하면 Kernel이 꺼진다..왜 이러지?\n",
    "# pltsize = 1\n",
    "# plt.figure(figsize = (10 * pltsize, pltsize))\n",
    "# for i in range(10):\n",
    "#     plt.subplot(1, 10, i + 1)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
    "#     plt.title('Class: ' + str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:53:42.219360Z",
     "start_time": "2021-01-19T11:53:42.209259Z"
    }
   },
   "outputs": [],
   "source": [
    "# AutoEncoder(AE) 모델 설계하기\n",
    "# PyTorch 모듈 내에 딥러닝 모델 관련 기본 함수를 포함하고 있는 nn.Module 클래스를 상속받는 AE 클래스를 정의\n",
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__() # nn.Module 내에 있는 메서드를 상속받아 사용\n",
    "        \n",
    "        # 인코더(encoder) 정의 >> nn.Sequential()을 통해 인코더 단위를 한 번에 정의\n",
    "        self.encoder = nn.Sequential(nn.Linear(28 * 28, 512),  # Input image data\n",
    "                                     nn.ReLU(),                # 첫 번째 Layer의 Output에 대해 ReLU 함수 적용해, 두 번째 Layer의 Input으로 전달\n",
    "                                     nn.Linear(512, 256),      # 두 번째 Layer의 Input 크기는 512, Output 크기는 256\n",
    "                                     nn.ReLU(),                # 두 번째 Layer의 Output에 대해 ReLU 함수 적용해, 두 번째 Layer의 Input으로 전달\n",
    "                                     nn.Linear(256, 32))       # 두 번째 Layer의 Input 크기는 256, Output 크기는 32\n",
    "        \n",
    "        # 디코더(decoder) 정의 >> nn.Sequential()을 통해 인코더 단위를 한 번에 정의\n",
    "        # 인코더(encoder)와 반대 방향으로 진행한다고 생각하면 쉬움(즉, 원복시키는 방향으로)\n",
    "        self.decoder = nn.Sequential(nn.Linear(32, 256),       # Latent Variable Vector를 Input으로 이용\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(256, 512),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(512, 28 * 28))\n",
    "    \n",
    "    # Forward Propagation(순전파) 정의\n",
    "    # 즉, 설계한 AutoEncoder의 인코더와 디코더에 데이터를 입력했을 때, Output을 계산하기까지의 과정을 나열한 것을 의미\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)        # Image 데이터를 사전에 정의한 인코더의 Input으로 이용하여 Latent Variable Vector를 생성\n",
    "        decoded = self.decoder(encoded)  # Latent Variable Vector 값이 저장된 encoded를 디코더의 Input으로 이용\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:53:42.250394Z",
     "start_time": "2021-01-19T11:53:42.224377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=32, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=784, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Optimizer, Objective Function 설정하기\n",
    "model = AE().to(DEVICE) # 'DEVICE' 장비를 이용하여 AE 모델을 완성하기 위해, 앞서 정의한 AE 모델을 기존에 선정한 'DEVICE'에 할당\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) # Adam Optimizer를 사용하고, Learning Rate은 0.001로 설정\n",
    "criterion = nn.MSELoss() # MSE를 모델 평가 지표로 사용\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:53:42.265370Z",
     "start_time": "2021-01-19T11:53:42.253033Z"
    }
   },
   "outputs": [],
   "source": [
    "# AE 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 (AE 모델을 학습)\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train() # 기존에 정의한 AE 모델을 '학습' 상태로 지정\n",
    "    # train_loader 내에 미니배치 단위로 저장된 데이터를 순서대로 이용하여 AE 모델을 학습\n",
    "    for batch_idx, (image, _) in enumerate(train_loader): # 입력 데이터를 target으로 학습\n",
    "        \n",
    "        # 기존에 정의한 AutoEncoder의 Input은 28 * 28 크기의 1차원 Layer이므로, 2차원 image 데이터를 1차원 데이터로 재구성해서 할당\n",
    "        image = image.view(-1, 28 * 28).to(DEVICE)   # 미니배치 내에 있는 image 데이터를 기존에 정의한 장비에 할당\n",
    "        target = image.view(-1, 28 * 28).to(DEVICE)  # 미니배치 내에 있는 image 데이터를 AE 모델의 Output과 비교하기 위해 기존에 정의한 장비에 할당\n",
    "        optimizer.zero_grad() # opimizer의 Gradient를 초기화\n",
    "        \n",
    "        encoded, decoded = model(image)    # 장비에 할당한 image 데이터를 AE 모델의 Input으로 이용\n",
    "        loss = criterion(decoded, target)\n",
    "        \n",
    "        loss.backward()   # Loss 값을 계산한 결과를 바탕으로 역전파를 통해 계산된 Gradient 값을 각 파라미터에 할당\n",
    "        optimizer.step()  # 각 파라미터에 할당된 Gradient 값을 이용하여 파라미터 값을 업데이트\n",
    "        \n",
    "        # 아래의 코드는 위 함수가 실행되는 과정을 모니터링하기 위한 코드\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), len(train_loader.dataset), \n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T11:53:42.280092Z",
     "start_time": "2021-01-19T11:53:42.269308Z"
    }
   },
   "outputs": [],
   "source": [
    "# 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 (학습의 진행 과정을 모니터링)\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()      # 학습 과정 또는 학습이 완료된 AE 모델을 학습 상태가 아닌 '평가' 상태로 지정\n",
    "    test_loss = 0     # 기존에 정의한 test_loader 내의 데이터를 이용하여 Loss 값을 계산하기 위해 '0'으로 임시 설정\n",
    "    real_image = []   # 학습 과정에서 AutoEncoder에 이용되는 실제 image 데이터 할당용\n",
    "    gen_image = []    # 학습 과정에서 AutoEncoder를 통해 생성되는 image 데이터 할당용\n",
    "    \n",
    "    # AE 모델을 평가하는 단계에서는 Gradient를 통해 파라미터 값이 업데이트 되는 현상을 방지하기 위해,\n",
    "    # torch.no_grad() 메서드를 통해 Gradient의 흐름을 억제\n",
    "    with torch.no_grad():\n",
    "        for image, _ in test_loader:\n",
    "            # 기존에 정의한 AutoEncoder의 Input은 28 * 28 크기의 1차원 Layer이므로, 2차원 image 데이터를 1차원 데이터로 재구성해서 할당\n",
    "            image = image.view(-1, 28 * 28).to(DEVICE)   # 미니배치 내에 있는 image 데이터를 기존에 정의한 장비에 할당\n",
    "            target = image.view(-1, 28 * 28).to(DEVICE)  # 미니배치 내에 있는 image 데이터를 AE 모델의 Output과 비교하기 위해 기존에 정의한 장비에 할\n",
    "            \n",
    "            encoded, decoded = model(image) # 장비에 할당한 image 데이터를 AE 모델의 Input으로 사용\n",
    "            test_loss += criterion(decoded, image).item() # test_loss 값 업데이트\n",
    "            \n",
    "            real_image.append(image.to('cpu'))   # 실제 이미지로 할당된 이미지를 real_image 리스트에 추가\n",
    "            gen_image.append(decoded.to('cpu'))  # AutoEncoder 모델을 통해 생성된 이미지를 gen_image 리스트에 추가\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset) # 현재까지 계산된 test_loss 값을 미니배치 개수만큼 나눠서 평균 Loss 값을 계산\n",
    "    \n",
    "    return test_loss, real_image, gen_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T12:02:35.401939Z",
     "start_time": "2021-01-19T11:53:42.284568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 0.197536\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.027059\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.025013\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.024060\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.017537\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.015437\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.016734\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.017141\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.016448\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.015457\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0005\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.016329\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.014880\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.012787\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.013817\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.014479\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.015783\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.013005\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.013383\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.011973\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.012770\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0004\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.015782\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.010897\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.013301\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.011695\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.012403\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.009605\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.014038\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.009719\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.013142\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.012763\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0004\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.011820\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.011926\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.012500\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.011809\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.010098\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.010638\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.012431\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.013201\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.009664\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.011699\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0003\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.009617\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.010338\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.013337\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.010723\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.010163\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.012930\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.010335\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.011030\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.010775\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.008804\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0003\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.010476\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.012388\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.009253\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.012988\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.010688\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.008771\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.010004\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.011456\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.009014\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.012022\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0003\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.009809\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.009924\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.009881\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.008020\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.010594\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.008513\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.011233\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.009500\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.010355\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.010405\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0003\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.010832\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.008668\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.009405\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.009668\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.008730\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.009136\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.009576\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.009366\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.009997\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.011275\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0003\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.009916\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.014081\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.009584\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.009391\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.008940\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.009967\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.009078\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.009414\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.009918\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.009852\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0003\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.008165\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.008681\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.010372\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.008550\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.011361\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.008360\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.010212\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.010184\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.008984\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.009168\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# AutoEncoder 학습을 실행하며 Test set의 Reconstruction Error 확인하기\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, real_image, gen_image = evaluate(model, test_loader)\n",
    "    print('\\n[EPOCH: {}], \\tTest Loss: {:.4f}'.format(epoch, test_loss))\n",
    "    \n",
    "#     # 실제 이미지와 생성된 이미지를 비교해서 학습의 진행도를 확인 >> 아래의 코드를 실행하면 Kernel이 꺼진다..왜 이러지?\n",
    "#     f, a = plt.subplots(2, 10, figsize = (10, 4))\n",
    "#     # real_image 출력(1행 1열 ~ 10열)\n",
    "#     for i in range(10):\n",
    "#         img = np.reshape(real_image[0][i], (28, 28))\n",
    "#         a[0][i].imshow(img, cmap = 'gray_r')\n",
    "#         a[0][i].set_xticks(())\n",
    "#         a[0][i].set_yticks(())\n",
    "#     # gen_image 출력(2행 1열 ~ 10열)\n",
    "#     for i in range(10):\n",
    "#         img = np.reshape(gen_image[0][i], (28, 28))\n",
    "#         a[1][i].imshow(img, cmap = 'gray_r')\n",
    "#         a[1][i].set_xticks(())\n",
    "#         a[1][i].set_yticks(())\n",
    "        \n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "DL_torch_cpu",
   "language": "python",
   "name": "dl_torch_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
