{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:33.190827Z",
     "start_time": "2021-01-14T06:51:32.494282Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서(Tensor)\n",
    "- 데이터를 표현하는 단위"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar\n",
    "- 상수 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:33.206185Z",
     "start_time": "2021-01-14T06:51:33.193417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "scalar1 = torch.tensor([1.])\n",
    "scalar2 = torch.tensor([3.])\n",
    "print(scalar1, scalar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:33.236692Z",
     "start_time": "2021-01-14T06:51:33.209255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.]) tensor([-2.]) tensor([3.]) tensor([0.3333])\n"
     ]
    }
   ],
   "source": [
    "add_scalar = scalar1 + scalar2\n",
    "sub_scalar = scalar1 - scalar2\n",
    "mul_scalar = scalar1 * scalar2\n",
    "div_scalar = scalar1 / scalar2\n",
    "print(add_scalar, sub_scalar, mul_scalar, div_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:33.251661Z",
     "start_time": "2021-01-14T06:51:33.238510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n",
      "tensor([-2.])\n",
      "tensor([3.])\n",
      "tensor([0.3333])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(scalar1, scalar2))\n",
    "print(torch.sub(scalar1, scalar2))\n",
    "print(torch.mul(scalar1, scalar2))\n",
    "print(torch.div(scalar1, scalar2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector\n",
    "- 하나의 값을 표현할 때, 2개 이상의 수치로 표현한 것\n",
    "- Scalar의 형태와 동일한 속성을 갖고 있지만, 여러 수치 값을 이용해 표현하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:33.266792Z",
     "start_time": "2021-01-14T06:51:33.254189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) tensor([4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "vector1 = torch.tensor([1., 2., 3.])\n",
    "vector2 = torch.tensor([4., 5., 6.])\n",
    "print(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:33.297558Z",
     "start_time": "2021-01-14T06:51:33.269799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.]) tensor([-3., -3., -3.]) tensor([ 4., 10., 18.]) tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "add_vector = vector1 + vector2\n",
    "sub_vector = vector1 - vector2\n",
    "mul_vector = vector1 * vector2\n",
    "div_vector = vector1 / vector2\n",
    "print(add_vector, sub_vector, mul_vector, div_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:33.312768Z",
     "start_time": "2021-01-14T06:51:33.300605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n",
      "tensor([-3., -3., -3.])\n",
      "tensor([ 4., 10., 18.])\n",
      "tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(vector1, vector2))\n",
    "print(torch.sub(vector1, vector2))\n",
    "print(torch.mul(vector1, vector2))\n",
    "print(torch.div(vector1, vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix\n",
    "- 2개 이상의 벡터 값을 통합해 구성된 값\n",
    "- 벡터 값 간 연산 속도를 빠르게 진행할 수 있는 선형 대수의 기본 단위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:33.328497Z",
     "start_time": "2021-01-14T06:51:33.316359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "matrix1 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "matrix2 = torch.tensor([[5., 6.], [7., 8.]])\n",
    "print(matrix1)\n",
    "print(matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:33.359367Z",
     "start_time": "2021-01-14T06:51:33.332042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "sum_matrix = matrix1 + matrix2\n",
    "sub_matrix = matrix1 - matrix2\n",
    "mul_matrix = matrix1 * matrix2\n",
    "div_matrix = matrix1 / matrix2\n",
    "print(sum_matrix)\n",
    "print(sub_matrix)\n",
    "print(mul_matrix)\n",
    "print(div_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:33.375672Z",
     "start_time": "2021-01-14T06:51:33.362397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(matrix1, matrix2))\n",
    "print(torch.sub(matrix1, matrix2))\n",
    "print(torch.mul(matrix1, matrix2))    # 백터 및 행렬 간 사칙연산 중 곱셈은 각 요소별로 계산됨\n",
    "print(torch.div(matrix1, matrix2))\n",
    "print(torch.matmul(matrix1, matrix2)) # 행렬 곱 연산을 수행할 때는 torch.matmul을 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "- Matrix를 2차원의 배열이라 표현할 수 있다면, Tensor는 2차원 이상의 배열이라 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:33.406684Z",
     "start_time": "2021-01-14T06:51:33.378859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n",
      "tensor([[[ 9., 10.],\n",
      "         [11., 12.]],\n",
      "\n",
      "        [[13., 14.],\n",
      "         [15., 16.]]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[[1., 2.], \n",
    "                         [3., 4.]], \n",
    "                        [[5., 6.], \n",
    "                         [7., 8.]]])\n",
    "tensor2 = torch.tensor([[[9., 10.], \n",
    "                         [11., 12.]], \n",
    "                        [[13., 14.], \n",
    "                         [15., 16.]]])\n",
    "print(tensor1)\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:33.422245Z",
     "start_time": "2021-01-14T06:51:33.412098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[10., 12.],\n",
      "         [14., 16.]],\n",
      "\n",
      "        [[18., 20.],\n",
      "         [22., 24.]]])\n",
      "tensor([[[-8., -8.],\n",
      "         [-8., -8.]],\n",
      "\n",
      "        [[-8., -8.],\n",
      "         [-8., -8.]]])\n",
      "tensor([[[  9.,  20.],\n",
      "         [ 33.,  48.]],\n",
      "\n",
      "        [[ 65.,  84.],\n",
      "         [105., 128.]]])\n",
      "tensor([[[0.1111, 0.2000],\n",
      "         [0.2727, 0.3333]],\n",
      "\n",
      "        [[0.3846, 0.4286],\n",
      "         [0.4667, 0.5000]]])\n"
     ]
    }
   ],
   "source": [
    "sum_tensor = tensor1 + tensor2\n",
    "sub_tensor = tensor1 - tensor2\n",
    "mul_tensor = tensor1 * tensor2 # 텐서 간 사칙연산 중 곱셈은 각 요소별로 계산됨\n",
    "div_tensor = tensor1 / tensor2\n",
    "print(sum_tensor)\n",
    "print(sub_tensor)\n",
    "print(mul_tensor)\n",
    "print(div_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:33.452848Z",
     "start_time": "2021-01-14T06:51:33.424792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[10., 12.],\n",
      "         [14., 16.]],\n",
      "\n",
      "        [[18., 20.],\n",
      "         [22., 24.]]])\n",
      "tensor([[[-8., -8.],\n",
      "         [-8., -8.]],\n",
      "\n",
      "        [[-8., -8.],\n",
      "         [-8., -8.]]])\n",
      "tensor([[[  9.,  20.],\n",
      "         [ 33.,  48.]],\n",
      "\n",
      "        [[ 65.,  84.],\n",
      "         [105., 128.]]])\n",
      "tensor([[[0.1111, 0.2000],\n",
      "         [0.2727, 0.3333]],\n",
      "\n",
      "        [[0.3846, 0.4286],\n",
      "         [0.4667, 0.5000]]])\n",
      "tensor([[[ 31.,  34.],\n",
      "         [ 71.,  78.]],\n",
      "\n",
      "        [[155., 166.],\n",
      "         [211., 226.]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(tensor1, tensor2))\n",
    "print(torch.sub(tensor1, tensor2))\n",
    "print(torch.mul(tensor1, tensor2))\n",
    "print(torch.div(tensor1, tensor2))\n",
    "print(torch.matmul(tensor1, tensor2)) # 텐서 간 텐서곱 연산을 수행할 때는 torch.matmul을 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "- Back Propagation(역전파)을 이용해 파라미터를 업데이트하는 방법은 Autograd 방식으로 쉽게 구현 가능\n",
    "- 아래의 간단한 예시를 통해 확인해보자.\n",
    "    - <예시 코드에 대한 설명>\n",
    "        - BATCH_SIZE 수만큼 데이터를 이용해 Output을 계산하고, BATCH_SIZE 수만큼 출력된 결과 값에 한 오차 값을 계산한다.\n",
    "        - BATCH_SIZE 수만큼 계산된 오차 값을 평균해서 Back Propagation을 적용하고 이를 바탕으로 파라미터를 업데이트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:51:34.076891Z",
     "start_time": "2021-01-14T06:51:33.455370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  100 \t Loss:  1010.9907836914062\n",
      "Iteration:  200 \t Loss:  12.613773345947266\n",
      "Iteration:  300 \t Loss:  0.2563186287879944\n",
      "Iteration:  400 \t Loss:  0.006329044234007597\n",
      "Iteration:  500 \t Loss:  0.00037221546517685056\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 가능 여부 파악\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "BATCH_SIZE = 64    # Input으로 이용되는 데이터의 개수\n",
    "INPUT_SIZE = 1000  # 입력층의 노드 수(즉, 입력 데이터의 크기)\n",
    "HIDDEN_SIZE = 100  # 입력층에서 은닉층으로 전달됬을 때, 은닉층의 노드 수\n",
    "OUTPUT_SIZE = 10    # 최종으로 출력되는 값의 벡터의 크기(보통 Output 크기는 최종으로 비교하고자 하는 레이블의 크기와 동일하게 설정)\n",
    "\n",
    "# x는 (64, 1000) 모양의 데이터\n",
    "x = torch.randn(BATCH_SIZE,\n",
    "                INPUT_SIZE,\n",
    "                device = DEVICE,\n",
    "                dtype = torch.float,\n",
    "                requires_grad = False) # x는 Input으로 이용되기 때문에 Gradient를 계산할 필요가 없음\n",
    "\n",
    "# y는 (64, 10) 모양의 데이터\n",
    "y = torch.randn(BATCH_SIZE,\n",
    "                OUTPUT_SIZE,\n",
    "                device = DEVICE,\n",
    "                dtype = torch.float,\n",
    "                requires_grad = False)\n",
    "\n",
    "# w1은 (1000, 100) 모양의 데이터\n",
    "w1 = torch.randn(INPUT_SIZE,\n",
    "                 HIDDEN_SIZE,\n",
    "                 device = DEVICE,\n",
    "                 dtype = torch.float,\n",
    "                 requires_grad = True) # Back Propagation을 통해 업데이트해야 하는 대상이므로, Gradient를 계산할 수 있도록 True로 설정\n",
    "\n",
    "# w2는 (100, 10) 모양의 데이터\n",
    "w2 = torch.randn(HIDDEN_SIZE,\n",
    "                 OUTPUT_SIZE,\n",
    "                 device = DEVICE,\n",
    "                 dtype = torch.float,\n",
    "                 requires_grad = True) # Back Propagation을 통해 업데이트해야 하는 대상이므로, Gradient를 계산할 수 있도록 True로 설정\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(1, 501):\n",
    "    # clamp(): 최솟값이 0이며 0보다 큰 값은 자기 자신을 갖게 되는 메서드\n",
    "    # 즉, 비선형 함수 ReLU()와 같은 역할\n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2) # mm은 matmul을 의미\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum() # pow(x): x 제곱 값 계산(즉, 지수를 취하는 기본 메서드)\n",
    "    if t % 100 == 0:\n",
    "        print('Iteration: ', t, '\\t', 'Loss: ', loss.item()) # 코드가 실행되는 과정을 모니터링하기 위함\n",
    "    loss.backward() # 각 파라미터 값에 대해 Gradient를 계산하고, 이를 통해 Back Propagation을 진행\n",
    "    \n",
    "    # 각 파라미터 값에 대해 Gradient를 계산한 결과를 이용해 파라미터 값을 업데이트할 때는\n",
    "    # 해당 시점의 Gradient 값을 고정한 후 업데이트를 진행한다.\n",
    "    with torch.no_grad(): # 코드가 실행되는 시점에서 Gradient 값을 고정한다는 의미\n",
    "        w1 -= learning_rate * w1.grad # Gradient 값을 고정한 상태에서 w1의 Gradient 값을 업데이트\n",
    "        w2 -= learning_rate * w2.grad # Gradient 값을 고정한 상태에서 w2의 Gradient 값을 업데이트\n",
    "        \n",
    "        # 각 파라미터 값을 업데이트했다면, 각 파라미터 값의 Gradient를 '0'으로 초기화\n",
    "        # 다음 Back Propagation을 진행할 때, Gradient 값을 loss.backward()를 통해 새로 계산하기 때문!\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "DL_torch_cpu",
   "language": "python",
   "name": "dl_torch_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
