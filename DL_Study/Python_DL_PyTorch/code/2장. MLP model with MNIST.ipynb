{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T11:48:13.667035Z",
     "start_time": "2021-01-14T11:48:11.597593Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn # 인공 신경망 모델을 설계할 때 필요한 함수를 모아 놓은 모듈\n",
    "import torch.nn.functional as F # torch.nn 모듈 중에서도 자주 이용되는 함수를 'F'로 지정\n",
    "from torchvision import transforms, datasets # 컴퓨터 비전 연구 분야에서 자주 이용하는 torchvision 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T11:48:13.682247Z",
     "start_time": "2021-01-14T11:48:13.669070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.7.1  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Mini-Batch와 Epoch>\n",
    "- 1개의 미니배치를 이용해 학습하는 횟수는 'Iteration'라고 한다.\n",
    "- 미니배치의 데이터 개수를 지정해준다면, Iteration은 전체 데이터 개수에서 1개의 미니배치를 구성하고 있는 데이터 개수를 나눠준 몫 만큼 Iteration을 진행한다.\n",
    "    - ex) 전체 데이터가 10,000개이고, 1,000개 데이터를 이용해 1개의 미니배치를 구성한다면 1 Epoch당 10회의 Iteration이 발생!\n",
    "\n",
    "<데이터 셋 불러오기 옵션>\n",
    "- 데이터를 다운로드할 때, 이미지 데이터에 대한 기본적인 전처리를 동시에 진행할 수 있다.\n",
    "- 아래의 예제에서는 ```transforms``` 함수 내 ```ToTensor()``` 메서드를 이용해 이미지 데이터를 'tensor' 형태로 변경하였다.\n",
    "    - 이미지 데이터의 경우, 한 픽셀은 0 ~ 255 범위의 스칼라 값으로 구성되어 있는데, 이를 0 ~ 1 범위로 변경해주는 정규화 과정을 진행해 준 것이다.\n",
    "    - **다층 퍼셉트론 모델이 포함된 인공 신경망 모델**은 Input 데이터 값의 크기가 커질수록 불안정하거나 과적합되는 방향으로 학습이 진행될 우려가 있기 때문에, **정규화 과정을 이용해 Input으로 이용하는 것을 권장**한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T11:48:18.480112Z",
     "start_time": "2021-01-14T11:48:13.684775Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e5818e59414844b856c8c4eff0c4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data/MNIST\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb45f7eb57a47e1aa988ceae009c850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data/MNIST\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd1fdb3210949559309d64daddef40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data/MNIST\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0858d9b6e4844497bd959271ed75f11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data/MNIST\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brian\\anaconda3\\envs\\DL_torch_cpu\\lib\\site-packages\\torchvision\\datasets\\mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 지정\n",
    "BATCH_SIZE = 32  # 미니배치 1개 단위에 대해 데이터가 32개로 구성되어 있는 것을 의미\n",
    "EPOCHS = 10      # 전체 데이터 셋을 10번 반복해 학습한다는 것을 의미 (즉, 전체 데이터를 이용해 학습을 진행한 횟수)\n",
    "\n",
    "# MNIST 데이터 셋 다운로드\n",
    "train_dataset = datasets.MNIST(root = '../data/MNIST',  # 데이터 셋이 저장될 장소를 지정\n",
    "                               train = True,            # 학습용 데이터 셋으로 지정 \n",
    "                               download = True,         # 인터넷 상에서 다운로드해서 이용할 것인지 여부\n",
    "                               transform = transforms.ToTensor()) # 데이터 셋을 tensor 형태로 변경\n",
    "test_dataset = datasets.MNIST(root = '../data/MNIST',\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "# 다운로드한 MNIST 데이터 셋을 미니배치 단위로 분리\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = BATCH_SIZE, # 미니배치 1개 단위를 구성하는 데이터의 개수\n",
    "                                           shuffle = True) # 데이터의 순서를 섞고자 할 때 이용(즉, 잘못된 방향으로 학습하는 것을 방지)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X_train\n",
    "    - **32개의 이미지 데이터가 1개의 미니배치를 구성**하고 있고 **가로 28개, 세로 28개의 픽셀로 구성**되어 있으며 **채널이 1이므로 Gray Scale(즉, 흑백)**로 이루어진 이미지 데이터이다.\n",
    "- y_train\n",
    "    - 32개의 이미지 데이터 각각에 label 값이 1개씩 존재하기 때문에 32개의 값을 갖고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T11:48:18.510372Z",
     "start_time": "2021-01-14T11:48:18.482629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인하기 (1)\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T11:48:18.525987Z",
     "start_time": "2021-01-14T11:48:18.511887Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 확인하기 (2) >> 코드를 실행하면 Kernel이 꺼진다..왜 이러지?\n",
    "# pltsize = 1\n",
    "# plt.figure(figsize = (10 * pltsize, pltsize))\n",
    "# for i in range(10):\n",
    "#     plt.subplot(1, 10, i + 1)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
    "#     plt.title('Class: ' + str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T11:48:18.541585Z",
     "start_time": "2021-01-14T11:48:18.527502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multi Layer Perceptron (MLP) 모델 설계하기\n",
    "# PyTorch 모듈 내에 딥러닝 모델 관련 기본 함수를 포함하고 있는 nn.Module 클래스를 상속받는 Net 클래스를 정의\n",
    "class Net(nn.Module):\n",
    "    def __init__(self): # Net 클래스의 인스턴스를 생성했을 때 지니게 되는 성질을 정의\n",
    "        super(Net, self).__init__()        # nn.Module 내에 있는 메서드를 상속받아 사용\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) # 첫 번째 Fully Connected Layer 정의\n",
    "        self.fc2 = nn.Linear(512, 256)     # 두 번째 Fully Connected Layer 정의\n",
    "        self.fc3 = nn.Linear(256, 10)      # 세 번째 Fully Connected Layer 정의\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # view 메서드를 통해 2차원 데이터를 784 크기의 1차원 데이터로 변환\n",
    "        \n",
    "        x = self.fc1(x)          # __init__()을 통해 정의한 첫 번째 Fully Connected Layer에 1차원으로 펼친 이미지 데이터를 통과\n",
    "        x = F.sigmoid(x)         # 비선형 함수인 sigmoid()를 이용하여 두 번째 Fully Connected Layer의 Input으로 계산\n",
    "        \n",
    "        x = self.fc2(x)          # __init__()을 통해 정의한 두 번째 Fully Connected Layer에 앞서 계산된 x를 통과\n",
    "        x = F.sigmoid(x)         # 비선형 함수인 sigmoid()를 이용하여 세 번째 Fully Connected Layer의 Input으로 계산\n",
    "        \n",
    "        x = self.fc3(x)                # __init__()을 통해 정의한 세 번째 Fully Connected Layer에 앞서 계산된 x를 통과\n",
    "        x = F.log_softmax(x, dim = 1)  # log_softmax()를 이용하여 최종 Output을 계산\n",
    "        # <참고>\n",
    "        # softmax가 아닌 log_softmax를 사용하는 이유는 MLP 모델이 역전파 알고리즘을 이용해 학습을 진행할 때,\n",
    "        # Loss 값에 대한 Gradient 값을 좀 더 원활하게 계산할 수 있기 때문!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T11:48:18.557208Z",
     "start_time": "2021-01-14T11:48:18.543097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Optimizer, Objective Function 설정하기\n",
    "model = Net().to(DEVICE) # 'DEVICE' 장비를 이용하여 MLP 모델을 완성하기 위해, 앞서 정의한 MLP 모델을 기존에 선정한 'DEVICE'에 할당\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5) # momentum은 Optimizer의 관성을 의미\n",
    "criterion = nn.CrossEntropyLoss() # MLP 모델의 Output 값과 원-핫 인코딩 값인 Label 값의 Loss를 CrossEntropy를 이용하여 계산\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T11:48:18.572847Z",
     "start_time": "2021-01-14T11:48:18.559743Z"
    }
   },
   "outputs": [],
   "source": [
    "# MLP 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 (MLP 모델을 학습)\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train() # 기존에 정의한 MLP 모델을 '학습' 상태로 지정\n",
    "    # train_loader 내에 미니배치 단위로 저장된 데이터를 순서대로 이용하여 MLP 모델을 학습\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE) # 미니배치 내에 있는 image 데이터를 이용하여 MLP 모델을 학습시키기 위해 기존에 정의한 장비에 할당\n",
    "        label = label.to(DEVICE) # 미니배치 내에 있는 image 데이터와 매칭된 label 데이터도 기존에 정의한 장비에 할당\n",
    "        \n",
    "        optimizer.zero_grad()    # opimizer의 Gradient를 초기화\n",
    "        \n",
    "        output = model(image)    # 장비에 할당한 이미지 데이터를 MLP 모델의 Input으로 이용\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        loss.backward()   # Loss 값을 계산한 결과를 바탕으로 역전파를 통해 계산된 Gradient 값을 각 파라미터에 할당\n",
    "        optimizer.step()  # 각 파라미터에 할당된 Gradient 값을 이용하여 파라미터 값을 업데이트\n",
    "        \n",
    "        # 아래의 코드는 위 함수가 실행되는 과정을 모니터링하기 위한 코드\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), len(train_loader.dataset), \n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T11:48:18.587954Z",
     "start_time": "2021-01-14T11:48:18.574851Z"
    }
   },
   "outputs": [],
   "source": [
    "# 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 (학습의 진행 과정을 모니터링)\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()   # 학습 과정 또는 학습이 완료된 MLP 모델을 학습 상태가 아닌 '평가' 상태로 지정\n",
    "    test_loss = 0  # 기존에 정의한 test_loader 내의 데이터를 이용하여 Loss 값을 계산하기 위해 '0'으로 임시 설정\n",
    "    correct = 0    # 학습 과정 또는 학습이 완료된 MLP 모델이 올바른 Class로 분류한 경우를 count 하기 위해 '0'으로 임시 설정\n",
    "    \n",
    "    # MLP 모델을 평가하는 단계에서는 Gradient를 통해 파라미터 값이 업데이트 되는 현상을 방지하기 위해,\n",
    "    # torch.no_grad() 메서드를 통해 Gradient의 흐름을 억제\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)  # 미니배치 내에 있는 image 데이터를 이용하여 MLP 모델을 검증하기 위해 기존에 정의한 장비에 할당\n",
    "            label = label.to(DEVICE)  # 미니배치 내에 있는 image 데이터와 매칭된 label 데이터도 기존에 정의한 장비에 할당\n",
    "            \n",
    "            output = model(image)     # 장비에 할당한 image 데이터를 MLP 모델의 Input으로 사용\n",
    "            test_loss += criterion(output, label).item()   # Loss 값 업데이트\n",
    "            \n",
    "            prediction = output.max(1, keepdim = True)[1]  # 가장 큰 벡터 값의 위치에 대응하는 클래스로 예측했다고 판단\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item() # 최종 예측이 올바른지 count\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset) # 현재까지 계산된 test_loss 값을 미니배치 개수만큼 나눠서 평균 Loss 값을 계산\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset) # 정확도 계산\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T11:51:12.525152Z",
     "start_time": "2021-01-14T11:48:18.589483Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.368604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brian\\anaconda3\\envs\\DL_torch_cpu\\lib\\site-packages\\torch\\nn\\functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 2.311240\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 2.320836\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 2.321223\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 2.316024\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 2.298596\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 2.333593\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 2.286542\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 2.273118\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 2.261302\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0699, \tTest Accuracy: 31.50 %\n",
      "]\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 2.238548\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 2.207880\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 2.190031\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 2.187530\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 1.896190\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 1.888133\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 1.520900\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 1.710994\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 1.266565\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 1.366138\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0389, \tTest Accuracy: 59.34 %\n",
      "]\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 1.235051\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 1.275618\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 1.131510\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.954072\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.903236\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.986940\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.893087\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.741330\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.591788\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.617549\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0233, \tTest Accuracy: 78.77 %\n",
      "]\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.623433\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.847190\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.621593\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.583924\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.606901\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.511512\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.562955\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.421691\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.640629\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.332936\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0173, \tTest Accuracy: 84.20 %\n",
      "]\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.600643\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.562220\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.668634\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.640688\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.625544\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.345291\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.378371\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.626432\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.676782\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.438654\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0142, \tTest Accuracy: 86.76 %\n",
      "]\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.526765\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.647512\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.350889\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.495952\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.508340\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.511411\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.431846\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.332463\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.426410\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.781659\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0127, \tTest Accuracy: 88.17 %\n",
      "]\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.352753\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.306885\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.377631\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.711200\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.314344\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.411360\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.267195\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.319283\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.363125\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.375913\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0121, \tTest Accuracy: 88.70 %\n",
      "]\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.296564\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.511612\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.551209\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.314274\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.532636\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.164301\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.334403\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.293719\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.228978\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.341672\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0112, \tTest Accuracy: 89.58 %\n",
      "]\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.308920\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.413405\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.321517\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.326755\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.248278\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.463624\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.360534\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.179719\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.245397\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.142168\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0108, \tTest Accuracy: 89.87 %\n",
      "]\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.331205\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.205222\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.319599\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.343310\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.227940\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.554840\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.577080\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.278909\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.465137\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.179478\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0104, \tTest Accuracy: 90.24 %\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# MLP 모델 학습을 실행하며 Train/Test set의 Loss 및 Test set Accuracy 확인하기\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200) # MLP 모델 학습 with SGD Optimizer\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)   # 각 epoch별로 출력되는 Loss 값과 accuracy 값을 계산\n",
    "    print('\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n]'.format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "DL_torch_cpu",
   "language": "python",
   "name": "dl_torch_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
