cvstat
help(numeric)
x <- dat[,1:6]
y <- dat[,7]
K <- 5  # K-fold
k <- 3  # kNN
cvf <- cvFolds(n, K)
cvstat <- numeric(K)  # numeric(length = 0)
help(numeric)
for(i in 1:K)
{
index <- cvf$subsets[cvf$which == i]
tr.x <- x[-index,]; tr.y = y[-index]
te.x <- x[index,]; te.y = y[index]
n2 <- length(te.y)
te.yhat <- knn(tr.x, te.x, tr.y, k)
cvstat[i] <- sum(te.y != te.yhat) / n2
}
tr.x
te.x
n2
te.yhat
mean(cvstat)
## Discriminant Analysis
dat <- read.table("D:/R_bigdata(advanced)/credit.txt", header=T)
dat
n <- nrow(dat)
n1 <- 500
n2 <- n - n1
# Training & Test data
idx <- sample(1:n, n1)
tr.dat <- dat[idx,]
te.dat <- dat[-idx,]
# LDA
library(MASS)
pred <- predict(fit.lda, newdata=te.dat)
fit.lda <- lda(Y ~ ., data=tr.dat)  # put every variables into X except for Y
pred <- predict(fit.lda, newdata=te.dat)
pred
yhat <- pred$class
yhat
pred
yhat <- pred$class
yhat
phat <- pred$posterior
phat
??posterior
help(posterior)
sum(te.dat$Y != yhat)/nrow(te.dat)
# QDA
fit.qda <- qda(Y ~ ., data=tr.dat)
pred <- predict(fit.qda, newdata=te.dat)
yhat
yhat <- pred$class
yhat
phat <- pred$posterior
phat
phat
sum(te.dat$Y != yhat)/nrow(te.dat)
## Logistic Regression ##
mydata <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
mydata
fit <- glm(admit~gre+gpa+factor(rank), data=mydata, family="binomial")  # family="binomial" : logistic regression
summary(fit)
pred_y <- ifelse(fit$fit>0.5, 1, 0)  # fit : predicted value
help(dnn)
??dnn
help(table)
table(mydata$admit, pred_y, dnn=c("Observed","Predicted"))  # dnn : put list names
table(mydata$admit, pred_y, dnn=c("Observed2","Predicted1"))  # dnn : put list names
table(mydata$admit, pred_y, dnn=c("Observed","Predicted"))  # dnn : put list names
new <- data.frame(gre=500, gpa=3.25, rank=1)
help(predict)
predict(fit, newdata=new, type="link")      # type="link" : calculate "predicted log odds"
predict(fit, newdata=new, type="response")  # type="response" : probability of pass the exam
pred <- predict(fit, newdata=new, type="response", se.fit=TRUE)
pred$fit
pred$se.fit
## Multinomial Logit regression
install.packages("nnet")
library(nnet)
diabetes <- read.csv(file("D:/R_bigdata(advanced)/diabetes.csv"), header=T)
diabetes
help(multinom)
summary(fit)
predict(fit, type="probs")  # type="probs" : probability
predict(fit, type="probs")  # type="probs" : probability
diabetes <- read.csv(file("D:/R_bigdata(advanced)/diabetes.csv"), header=T)
diabetes
fit <- multinom(CC~RW+IR+SSPG, data=diabetes)
summary(fit)
predict(fit, type="probs")  # type="probs" : probability
pred_CC <- predict(fit, type="class")  # type="class" : classification
pred_CC
table(diabetes$CC, pred_CC, dnn=c("Observed","Predicted"))
# Q1 -  Ridge & Lasso regression
library(MASS)
Boston
x <- as.matrix(Boston[,1:13])
x
y <- Boston[,14]
y
cv.out1 <- cv.glmnet(x, y, alpha=0, nfolds=10)
View(cv.out1)
opt.lam1 <- cv.out1$lambda.min
fit <- glm(x, y, alpha=0, lambda=opt.lam1)
install.packages("glmnet")
install.packages("glmnet")
library(glmnet)
x <- as.matrix(Boston[,1:13])
y <- Boston[,14]
cv.out1 <- cv.glmnet(x, y, alpha=0, nfolds=10)
opt.lam1 <- cv.out1$lambda.min
fit <- glm(x, y, alpha=0, lambda=opt.lam1)
cv.out1
opt.lam1 <- cv.out1$lambda.min
opt.lam1
fit <- glm(x, y, alpha=0, lambda=opt.lam1)
fit <- glm(x, y, alpha=0, lambda=opt.lam1)
opt.lam1 <- cv.out1$lambda.min
fit <- glm(x, y, alpha=0, lambda=opt.lam1)
cv.out1 <- cv.glmnet(x, y, alpha=0, nfolds=5)
opt.lam1 <- cv.out1$lambda.min
fit <- glm(x, y, alpha=0, lambda=opt.lam1)
cv.out1 <- cv.glmnet(x, y, alpha=0, nfolds=10)
opt.lam1 <- cv.out1$lambda.min
fit <- glm(x, y, alpha=0, lambda=opt.lam1)
fit <- glmnet(x, y, alpha=0, lambda=opt.lam1)
fit
fit$beta
cv.out1 <- cv.glmnet(x, y, alpha=0, nfolds=10)
opt.lam1 <- cv.out1$lambda.min
fit <- glmnet(x, y, alpha=0, lambda=opt.lam1)
yhat <- predict(fit, s=opt.lam1, newx=x)
yhat
# Lasso regression
cv.out2 <- cv.glmnet(x, y, alpha=1, nfolds=10)
opt.lam2 <- cv.out2$lambda.min
fit <- glmnet(x, y, alpha=1, lambda=opt.lam2)
fit$beta
fit
# Ridge regression
cv.out1 <- cv.glmnet(x, y, alpha=0, nfolds=5)
opt.lam1 <- cv.out1$lambda.min
fit <- glmnet(x, y, alpha=0, lambda=opt.lam1)
fit
fit$beta
# Ridge regression
cv.out1 <- cv.glmnet(x, y, alpha=0, nfolds=10)
opt.lam1 <- cv.out1$lambda.min
fit <- glmnet(x, y, alpha=0, lambda=opt.lam1)
fit
fit$beta
yhat <- predict(fit, s=opt.lam1, newx=x)
yhat
# Lasso regression
cv.out2 <- cv.glmnet(x, y, alpha=1, nfolds=10)
opt.lam2 <- cv.out2$lambda.min
fit <- glmnet(x, y, alpha=1, lambda=opt.lam2)
fit
fit$beta
## Q2 - LDA & QDA, Logistic regression
bankruptcy <- read.csv("D:/R_bigdata(advanced)/bankruptcy.csv", header=T)
bankruptcy
n <- nrow(bankruptcy)
n
View(bankruptcy)
n1 <- 40
n2 <- n - n1
idx <- sample(1:n, n1)
tr.bank <- bankruptcy[idx,]
te.bank <- bankruptcy[-idx,]
library(MASS)
fit.lda <- lda(Y ~ ., data=bankruptcy)
pred <- predict(fit.lda, newdata=te.bank)
yhat <- pred$class
yhat
pred
fit.lda <- lda(Y ~ ., data=bankruptcy)
pred <- predict(fit.lda, newdata=te.bank)
pred
yhat <- pred$class
yhat
pred_y <- pred$posterior
pred_y
phat <- pred$posterior
phat
sum(te.bank$Y != yhat)/nrow(te.bank)
# QDA
library(MASS)
fit.qda <- qda(Y ~ ., data=bankruptcy)
pred <- predict(fit.qda, newdata=te.bank)
yhat <- pred$class
yhat
phat <- pred$posterior
phat
sum(te.bank$Y != yhat)/nrow(te.bank)
# Logistic Regression
fit <- glm(Y~X1+X2+X3, data=bankruptcy, family="binomial")
summary(fit)
pred_y <- ifelse(fit$fit>0.5, 1, 0)
table(bankruptcy$Y, pred_y, dnn=c("Observed", "Predicted"))
pred_y <- ifelse(fit$fit>0.7, 1, 0)
table(bankruptcy$Y, pred_y, dnn=c("Observed", "Predicted"))
pred_y <- ifelse(fit$fit>0.3, 1, 0)
table(bankruptcy$Y, pred_y, dnn=c("Observed", "Predicted"))
pred_y <- ifelse(fit$fit>0.5, 1, 0)
table(bankruptcy$Y, pred_y, dnn=c("Observed", "Predicted"))
pred$fit     # shows probability of pass the exam
pred$se.fit  # se.fit : calculate the "standard error"
2/66
# Q2_2
new <- data.frame(X1=40, X2=6, X3=1.8)
predict(fit, newdata=new, type="link")
pred <- predict(fit, newdata=new, type="response", se.fit=TRUE)
pred$fit
pred$se.fit
pred
pred$se.fit
# Q2_2
new <- data.frame(X1=40, X2=6, X3=1.8)
predict(fit, newdata=new, type="link")
pred <- predict(fit, newdata=new, type="response", se.fit=TRUE)
pred$fit
pred <- predict(fit.qda, newdata=te.bank)
pred
pred <- predict(fit.qda, newdata=new)
pred
new <- data.frame(X1=40, X2=6, X3=1.8)
predict(fit, newdata=new, type="link")
pred <- predict(fit, newdata=new, type="response", se.fit=TRUE)
pred
pred$fit
pred
# Q2_2
new <- data.frame(X1=40, X2=6, X3=1.8)
pred <- predict(fit.qda, newdata=new)
pred
# Q2_2
new <- data.frame(X1=40, X2=6, X3=1.8)
pred <- predict(fit.lda, newdata=new)
pred
new <- data.frame(X1=40, X2=6, X3=1.8)
pred <- predict(fit.qda, newdata=new)
pred
# Q2_2
new <- data.frame(X1=40, X2=6, X3=1.8)
pred1 <- predict(fit.lda, newdata=new)
pred1
new <- data.frame(X1=40, X2=6, X3=1.8)
pred2 <- predict(fit.qda, newdata=new)
pred2
new <- data.frame(X1=40, X2=6, X3=1.8)
predict(fit, newdata=new, type="link")
pred3 <- predict(fit, newdata=new, type="response", se.fit=TRUE)
pred3$fit
pred3$se.fit
# Data
train <- read.table("C:/Users/SKKU/Documents/train.txt", header=T)
# Data
train <- read.table("D:/R_bigdata(advanced)/train.txt", header=T)
test <- read.table("D:/R_bigdata(advanced)/test.txt", header=T)
# Decision Tree
install.packages("tree")
library(tree)
help(tree)
tree.fit <- tree(medv~., data=train)
summary(tree.fit)
plot(tree.fit)
text(tree.fit, pretty=0)
# Pruning
help(cv.tree)
# Pruning
cv.fit <- cv.tree(tree.fit)
cv.fit
# Size corresponding to the smallest cv statistic.
sz <- cv.fit$size[which(cv.fit$dev == min(cv.fit$dev))]
sz
# Alpha corresponding to the smallest cv statistic.
cv.fit$k[which(cv.fit$dev == min(cv.fit$dev))]
prune.fit <- prune.tree(tree.fit, best=sz)
help(tree)
help(prune.tree)
plot(prune.fit)
text(prune.fit, pretty=0)
plot(tree.fit)
text(tree.fit, pretty=0)
plot(prune.fit)
text(prune.fit, pretty=0)
plot(tree.fit)
text(tree.fit, pretty=0)
plot(prune.fit)
text(prune.fit, pretty=0)
yhat <- predict(prune.fit, newdata=test)
mean((test$medv - yhat)^2)  # Test MSE
# Random Forests
install.packages("randomForest")
library(randomForest)
View(train)
rf.fit <- randomForest(medv~., data=train, mtry=4, importance=TRUE)
# mtry=value of "m", importance="important variables"
rf.fit
yhat.rf <- predict(rf.fit, newdata=test)
mean((test$medv - yhat.rf)^2)  # Test MSE
# Importance of variables
importance(rf.fit)  # calculate the importance of variables
varImpPlot(rf.fit)  # draw the importance of variables
# Data
train <- read.table("D:/R_bigdata(advanced)/train.txt", header=T)
test <- read.table("D:/R_bigdata(advanced)/test.txt", header=T)
# Scaling
maxs <- apply(train, 2, max)  # apply(matrix, 1(row)/2(column), function)
mins <- apply(train, 2, min)
help(scale)
# Neural networks
install.packages('neuralnet')
library(neuralnet)
nn <- neuralnet(medv ~ crim + zn + indus + chas + nox + rm + age +
dis + rad + tax + ptratio + black + lstat,
data=train.s, hidden=c(5,3), linear.output=T)
train.s <- as.data.frame(scale(train, center = mins, scale = maxs - mins))
test.s <- as.data.frame(scale(test, center = mins, scale = maxs - mins))
nn <- neuralnet(medv ~ crim + zn + indus + chas + nox + rm + age +
dis + rad + tax + ptratio + black + lstat,
data=train.s, hidden=c(5,3), linear.output=T)
nn
# Result
plot(nn)
#Prediction
pr.nn <- compute(nn, test.s[,1:13]) # medvhat with scaling
medvhat <- pr.nn$net.result*(maxs[14]-mins[14])+mins[14] # medvhat with no scaling
pr.nn
plot(pr.nn)
medvhat
# Test error
sum((test$medv - medvhat)^2)/nrow(test.s)
# Data
total <- read.table("D:/R_bigdata(advanced)/train.txt", header=T)
p <- ncol(total)
p
pca <- princomp(total, cor=T)  # cor=T : No needs to standardization in data
pca
pca <- princomp(total, cor=F)  # cor=T : No needs to standardization in data
pca
summary(pca)
# Scree plot(to find "elbow point")
pev <- pca$sdev^2 / sum(pca$sdev^2)
pev
plot(1:p, pev, type="l", xlab='# of PCs', ylab='PEV')
plot(1:p, pev, xlab='# of PCs', ylab='PEV')
plot(1:p, pev, type="l", xlab='# of PCs', ylab='PEV')
# PC scores for Test data
test <- read.table("D:/R_bigdata(advanced)/test.txt", header=T)
te.pc <- predict(pca, test)
te.pc
pca$scores
# Data
iris
x <- iris[,1:4]
km <- kmeans(x, 3, nstart=20)  # nstart="numbers of repeating k-means algorithm"
km
# result
km$cluster
iris[km$cluster==1,] # shows data which is "cluster=1"
iris[,km$cluster==1] # shows data which is "cluster=1"
plot(x, col=(km$cluster+1), pch=20, cex=2)
km
plot(x, col=(km$cluster+1), pch=20, cex=2)
help(plot)
# Hierarchical Clustering (Agglomerative) ----------------------
hc.s <- hclust(dist(x), method="single")
cutree(hc.s, 3)
hc.c <- hclust(dist(x), method="complete")
cutree(hc.c, 3)
hc.a <- hclust(dist(x), method="average")
cutree(hc.a, 3)
# dendogram
par(mfrow=c(1,3))
plot(hc.s, main='Single linkage', cex=0.5)
plot(hc.c, main='Complete linkage', cex=0.5)
plot(hc.a, main='Average linkage', cex=0.5)
install.packages("tm")
install.packages("SnowballC")
library(tm)
library(SnowballC)
# Locate and load the Corpus.
cname <- file.path(".", "data", "txt")  # folder containg documents
help(file.path)
docs <- Corpus(DirSource(cname))        # Create corpus
## Q3 ##
data <- read.table("D:/R_bigdata(advanced)/credit.txt", header=T)
data
n <- nrow(data)
n1 <- 500
n2 <- n - n1
idx <- sample(1:n, n1)
tr.dat <- data[idx,]
tr.data <- data[idx,]
te.data <- data[-idx,]
library(tree)
tree.fit <- tree(Y~., data=tr.data)
summary(tree.fit)
82/500
plot(tree.fit)
text(tree.fit, pretty=0)
par(mfrow=c(1,1))
plot(tree.fit)
text(tree.fit, pretty=0)
cv.fit <- cv.tree(tree.fit)
cv.fit
sz <- cv.fit$size[which(cv.fit$dev == min(cv.fit$dev))]
sz
cv.fit$k[which(cv.fit$dev == min(cv.fit$dev))]
prune.fit <- prune.tree(tree.fit, best=sz)
plot(prune.fit)
text(prune.fit, pretty=0)
yhat <- predict(prune.fit, newdata=te.data)
mean((te.data$Y - yhat)^2)  # Test MSE
yhat <- predict(prune.fit, newdata=te.data)
mean((te.data$Y - yhat)^2)  # Test MSE
# Random Forest
library(randomForest)
rf.fit <- randomForest(Y~., data=tr.data, mtry=2, importance=TRUE)
rf.fit
rf.fit <- randomForest(Y~., data=tr.data, mtry=3, importance=TRUE)
rf.fit
rf.fit <- randomForest(Y~., data=tr.data, mtry=4, importance=TRUE)
rf.fit
rf.fit <- randomForest(Y~., data=tr.data, mtry=2, importance=TRUE)
rf.fit
rf.fit <- randomForest(Y~., data=tr.data, mtry=3, importance=TRUE)
rf.fit
yhat.rf <- predict(rf.fit, newdata=te.data)
mean((te.data$Y - yhat.rf)^2)  # Test MSE
mean((te.data$factor(Y) - yhat.rf)^2)
maxs <- apply(tr.data, 2, max)
mins <- apply(tr.data, 2, min)
train.s <- as.data.frame(scale(tr.data, center = mins, scale = maxs - mins))
test.s <- as.data.frame(scale(te.data, center = mins, scale = maxs - mins))
data
idx <- sample(1:n, n1)
tr.data <- data[idx,]
te.data <- data[-idx,]
maxs <- apply(tr.data, 2, max)
mins <- apply(tr.data, 2, min)
train.s <- as.data.frame(scale(tr.data, center = mins, scale = maxs - mins))
test.s <- as.data.frame(scale(te.data, center = mins, scale = maxs - mins))
maxs
tr.data <- data[idx,]
## Q3_3 ##
USArrests
## Q3_3 ##
data_pca <- USArrests
pca <- princomp(data_pca, cor=T)
summary(pca)
pca$scores[,1:3]
pev <- pca$sdev^2 / sum(pca$sdev^2)
pev
plot(1:p, pev, type="l", xlab='# of PCs', ylab='PEV')
plot(1:3, pev, type="l", xlab='# of PCs', ylab='PEV')
plot(1:4, pev, type="l", xlab='# of PCs', ylab='PEV')
## Q3_4 ##
# K-means clustering
data_km <- USArrests
data_km
x <- data_km[, c(2,3,4,5)]
x <- data_km[,c=(2,3,4,5)]
x <- data_km[,2:5]
x
x <- data_km[,-1]
x
data_km
x <- data_km[,-5]
x
data_km
km <- kmeans(data_km, 3, nstart=20)
km
km <- kmeans(data_km, 4, nstart=20)
km
km <- kmeans(data_km, 5, nstart=20)
km
km$cluster
km1 <- kmeans(data_km, 3, nstart=20)  # k=3
km1
km1$cluster
km2 <- kmeans(data_km, 4, nstart=20)  # k=4
km2
km2$cluster
km3 <- kmeans(data_km, 5, nstart=20)  # k=5
km3
km3$cluster
plot(data_km, col=(km1$cluster+1), pch=20, cex=2)
plot(data_km, col=(km1$cluster+1), pch=20, cex=2)
plot(data_km, col=(km2$cluster+1), pch=20, cex=2)
plot(data_km, col=(km3$cluster+1), pch=20, cex=2)
## Q3_5 ##
# Hierarchical Clustering (Agglomerative)
hc.a <- hclust(dist(data_km), method="average")
cutree(hc.a)
cutree(hc.a, 3)
plot(hc.a, main="Average Linkage")
plot(hc.a, main="Average Linkage", cex=0.5)
plot(hc.a, main="Average Linkage", cex=0.3)
plot(hc.a, main="Average Linkage", cex=0.7)
plot(hc.a, main="Average Linkage")
plot(hc.a, main="Average Linkage", cex=0.8)
hc.a2 <- hclust(dist(data_km), method="average")
cutree(hc.a2, 4)
plot(hc.a2, main="Average Linkage", cex=0.8)
hc.a2 <- hclust(dist(data_km), method="average")
cutree(hc.a2, 4)
plot(hc.a2, main="Average Linkage", cex=0.8)
