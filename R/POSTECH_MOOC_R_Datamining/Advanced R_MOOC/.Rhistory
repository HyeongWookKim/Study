dat1 <- wages1833
dat1 <- na.omit(dat1)
str(dat1)
# calculate distance between each nodes
dist_data <- dist(dat1)
dis_data
dist_data
# prepare hierarchical cluster
# complete linkage method
hc_a <- hclust(dist_data, method="complete")
hc_a
plot(hc_a, hang=-1, cex=0.3, main="complete")
plot(hc_a, hang=-1, cex=0.8, main="complete")
# average linkage method
# check how different from complete method
hc_c <- hclust(dist_data, method="average")
hc_c
plot(hc_c, hang=-1, cex=0.8, main="average")
plot(hc_c, hang=-2, cex=0.8, main="average")
plot(hc_c, cex=0.8, main="average")
plot(hc_c, hang=-1, cex=0.8, main="average")
# Ward's method
hc_w <- hclust(dist_data, method="Ward.D2")
# Ward's method
hc_w <- hclust(dist_data, method="ward.D2")
hc_w
plot(hc_c, hang=-1, cex=0.8, main="Ward's method")
# install.package & set library
# install.packages("DAAG")
library(DAAG)
# preprocessing
dat1 <- wages1833
dat1 <- na.omit(dat1)
head(dat1, n=5)
# to choose the optimal k
install.packages("factoextra")
library(factoextra)
fviz_nbclust(dat1, kmeans, method="wss")
fviz_nbclust(dat1, kmeans, method="silhouette")
fviz_nbclust(dat1, kmeans, method="gap_stat")
fviz_nbclust(dat1, kmeans, method="wss")
# compute kmeans
set.seed(123)
km <- kmeans(dat1, 3, nstart=25)
km
# visualize
fviz_cluster(km, data=dat1, ellipse.type="convex", repel=TRUE)
# visualize
fviz_cluster(km, data=dat1, ellipse.type="convex", repel=FALSE)
# visualize
fviz_cluster(km, data=dat1, ellipse.type="convex", repel=TRUE)
# compute PAM
library("cluster")
pam_out <- pam(dat1, 3)
pam_out
# freq of each cluster
table(pam_out$clustering)
# visualize
fviz_cluster(pam_out, data=dat1, ellipse.type="convex", repel=TRUE)
# association rule analysis package
install.packages("arules")
library(arules)
# data import-> make transaction data
dvd1 <- read.csv("dvdtrans.csv")
dvd1
dvd.list <- split(dvd1$Item, dvd1$ID)
dvd.list
dvd.trans<-as(dvd.list,"transactions")
dvd.trans
inspect(dvd.trans)
View(dvd.trans)
# summary of dvd.trans
summary(dvd.trans)
# for running dvdtras data
dvd_rule <- apriori(dvd.trans,
parameter=list(support=0.2, confidence = 0.20, minlen = 2))
dvd_rule
summary(dvd_rule)
inspect(dvd_rule)
# Bar chart for support>0.2
itemFrequencyPlot(dvd.trans, support=0.2, main="item for support>=0.2", col="green")
# association rule analysis package
# install.packages("arules")
library(arules)
#association rule analysis
data("Groceries")
View(Groceries)
summary(Groceries)
# Bar chart for item with support>=5%
itemFrequencyPlot(Groceries, supp=0.05, main="item for support>=5%", col="green", cex=0.8)
# Bar chart for the top support 20 items
itemFrequencyPlot(Groceries, topN=20, main="support top 20 items", col="coral", cex=0.8)
# Bar chart for the top support 20 items
itemFrequencyPlot(Groceries, topN=10, main="support top 20 items", col="coral", cex=0.8)
# Bar chart for the top support 20 items
itemFrequencyPlot(Groceries, topN=20, main="support top 20 items", col="coral", cex=0.8)
# Association rule with support>5%, confidence>20% in minimum length 2
Grocery_rule <- apriori(data=Groceries,
parameter=list(support=0.05,
confidence = 0.20,
minlen = 2))
Grocery_rule
#analyzing result
summary(Grocery_rule)
inspect(Grocery_rule)
# sorting by Lift
inspect(sort(Grocery_rule, by="lift"))
# searching association for interesting items
rule_interest <- subset(Grocery_rule, items %in% c("yogurt","whole milk"))
inspect(rule_interest)
rule_interest3 <- subset(Grocery_rule, items %pin% c("yogurt"))
inspect(rule_interest3)
rule_interest5 <- subset(Grocery_rule, items %pin% c("other") & confidence > 0.25)
inspect(rule_interest5)
# save as dataframe
Grocery_rule_df <- as(Grocery_rule, "data.frame")
Grocery_rule_df
#saving results as csv file
write(Grocery_rule, file="Grocery_rule.csv",
sep=",",
quote=TRUE,
row.names=FALSE)
View(Grocery_rule)
# sorting by Lift
inspect(sort(Grocery_rule, by="support"))
# Bar chart for item with support>=5%
itemFrequencyPlot(Groceries, supp=0.05, main="item for support>=5%", col="green", cex=0.8)
# Bar chart for the top support 20 items
itemFrequencyPlot(Groceries, topN=20, main="support top 20 items", col="coral", cex=0.8)
rule_interest7 <- subset(Grocery_rule, items %in% c("yogurt","rolls/buns") & support=0.04 & confidence=0.2 & minlen=2)
Grocery_rule2 <- apriori(data=Groceries,
parameter=list(support=0.04,
confidence = 0.20,
minlen = 2))
rule_interest7 <- subset(Grocery_rule2, items %in% c("yogurt","rolls/buns"))
inspect(rule_interest7)
# Sequence Pattern analysis package
install.packages("arulesSequences")
library(arulesSequences)
data(zaki)
summary(zaki)
# to see data frame
as(zaki,"data.frame" )
seq_rule1 <- cspade(zaki,
parameter=list(support = 0.3,
maxsize = 5, maxlen =4),
control = list(verbose=TRUE))
#analyzing results
summary(seq_rule1)
as(seq_rule1, "data.frame")
#selecting rules(size>2)
seq_rule1_df <- as(seq_rule1, "data.frame")
seq_rule1_size <- size(seq_rule1)
seq_rule1_df <- cbind(seq_rule1_df, seq_rule1_size)
seq_rule_1_df_size2 <- subset(seq_rule1_df,
subset=seq_rule1_size>=2)
seq_rule_1_df_size2
seq_rule1_df
# Question 2(exercise)
seq_rule2 <- cspade(zaki,
parameter=list(support = 0.3,
maxsize = 3),
control = list(verbose=TRUE))
as(seq_rule2, "data.frame")
maxsize = 3)
# Question 2(exercise)
seq_rule2 <- cspade(zaki,
parameter=list(support = 0.3,
maxsize = 3))
as(seq_rule2, "data.frame")
, maxlen =4
# Question 2(exercise)
seq_rule2 <- cspade(zaki,
parameter=list(support = 0.3,
maxsize = 3, maxlen =4),
control = list(verbose=TRUE))
# Question 2(exercise)
seq_rule2 <- cspade(zaki,
parameter=list(support = 0.3,
maxsize = 3, maxlen =4),
control = list(verbose=TRUE))
as(seq_rule2)
as(seq_rule2, "data.frame")
seq_rule2_df <- as(seq_rule2, "data.frame")
seq_rule2_size <- size(seq_rule2)
seq_rule2_df <- cbind(seq_rule2_df, seq_rule2_size)
seq_rule2_df
seq_rule_2_df_size <- subset(seq_rule2_df,
subset=seq_rule2_size>=3)
seq_rule_1_df_size
seq_rule_2_df_size
#input data(iris)
iris <- read.csv(file="iris.csv")
attach(iris)
head(iris)
# Check correlation
cor(iris[1:4])
# 1.PCA(center=T -> mean=0, scale.=T -> variance=1)
ir.pca <- prcomp(iris[,1:4], center=T, scale.=T)
# 1.PCA(center=T -> mean=0, scale.=T -> variance=1)
ir.pca <- prcomp(iris[,1:4], center=T, scale=T)
ir.pca
summary(ir.pca)
# 2.scree plot : to choose the number of components
plot(ir.pca, type="l")
# either way to draw scree plot
screeplot(ir.pca)
View(ir.pca)
# 3. calculate component=x_data%*% PCA weight
PRC <- as.matrix(iris[,1:4]) %*% ir.pca$rotation
head(PRC)
# 4. classification using principal components
# make data with components
iris.pc <- cbind(as.data.frame(PRC), Species)
head(iris.pc)
1
# 5. support vector machine
# install.packages("e1071")
library(e1071)
# classify all data using PC1-PC4 using support vector machine
m1 <- svm(Species ~., data = iris.pc, kernel="linear")
# m2<- svm(Species ~PC1+PC2, data = iris.pc, kernel="linear")
summary(m1)
# predict class for all data
x <- iris.pc[,-5]
pred <- predict(m1, x)
pred
# check accuracy between true class and predicted class
y <- iris.pc[,5]
table(pred, y)
# wine data
wine <- read.csv(file="wine_aroma.csv")
# install package for Partial Least Square
install.packages("pls")
library(pls)
# example PLS with gasoline data
data(gasoline)
help("gasoline")
head(gasoline)
attach(gasoline)
# descriptive statistics
par(mfrow=c(1,1))
hist(octane, col=3)
hist(octane, col=2)
hist(octane, col=3)
summary(octane)
# pls function
help(plsr)
# train and test set
gasTrain <- gasoline[1:50, ]
gasTest <- gasoline[51:60,]
# 1.check how many principal components
ga.pca <- prcomp(gasoline$NIR, center=T, scale=F)
ga.pca
summary(ga.pca)
plot(ga.pca, type="l")
# pls model by training set (find LV by leave-one-out)
# 1. start with 10 component PLS model
gas1 <- plsr(octane ~ NIR, ncomp = 10, data = gasTrain, validation="LOO")
summary(gas1)
# 2. to choose the number of components
plot(RMSEP(gas1), legendpos = "topright", pch=46, cex=1.0, main="Cross-validation for # of LV")
# 3. Display the PLS model with LV=2
# scatterplot with true and predicted
plot(gas1, ncomp=2, asp=1, line = TRUE, cex=1.5, main="Measured vs Predicted", xlab="Measured")
# Check explained variances proportion for X
explvar(gas1)
# 4. predicted Y for test data
ypred <- predict(gas1, ncomp = 2, newdata=gasTest)
# actual value of y
y <- gasoline$octane[51:60]
# check : RMSEP for test data
sqrt((sum(y-ypred)^2)/10)
# output of y and predicted y
out1 <- cbind(y, ypred)
# data exporting
write.csv(out1, file="out1.csv", row.names=FALSE)
#regression(PC1,PC2)
fit1 <- lm(Species~PC1+PC2, data=iris.pc)
fit1
# input data(iris)
iris <- read.csv(file="iris.csv")
attach(iris)
head(iris)
# Check correlation
cor(iris[1:4])
# 1.PCA(center=T -> mean=0, scale=T -> variance=1)
ir.pca <- prcomp(iris[,1:4], center=T, scale=T)
ir.pca
summary(ir.pca)
# 2.scree plot : to choose the number of components
plot(ir.pca, type="l")
# either way to draw scree plot
screeplot(ir.pca)
# 3. calculate component=x_data%*% PCA weight
PRC <- as.matrix(iris[,1:4]) %*% ir.pca$rotation
head(PRC)
# 4. classification using principal components
# make data with components
iris.pc <- cbind(as.data.frame(PRC), Species)
head(iris.pc)
# 5. support vector machine
# install.packages("e1071")
library(e1071)
# classify all data using PC1-PC4 using support vector machine
m1 <- svm(Species ~., data = iris.pc, kernel="linear")
# m2<- svm(Species ~PC1+PC2, data = iris.pc, kernel="linear")
summary(m1)
# predict class for all data
x <- iris.pc[,-5]
pred <- predict(m1, x)
# check accuracy between true class and predicted class
y <- iris.pc[,5]
table(pred, y)
#regression(PC1,PC2)
fit1 <- lm(Species~PC1+PC2, data=iris.pc)
fit1
#measuring accuracy
fit1_pred <- predict(fit1, newdata=iris.pc)
b <- round(fit1_pred)
b[b==0|b==1] <- "setosa"
b[b==2] <- "Versicolor"
b[b==3] <- "Virginica"
a <- iris[,5]
table(b,a)
# require mxnet
install.packages("https://github.com/jeremiedb/mxnet_winbin/raw/master/mxnet.zip",repos = NULL)
library(mxnet)
library(XML)
library(DiagrammeR)
# If you have Error message "no package called XML or DiagrmmeR", then install
install.packages("XML")
install.packages("XML")
install.packages("DiagrammeR")
library(DiagrammeR)
#install.packages('devtools')
library(devtools)
# read csv file
iris <- read.csv("iris.csv")
attach(iris)
# change to numeric from character variable : Species
iris[,5] = as.numeric(iris[,5])-1
table(Species)
# check the data
head(iris, n=10)
# split train & test dataset
# training (n=100)/ test data(n=50)
set.seed(1000)
N <- nrow(iris)
tr.idx <- sample(1:N, size=N*2/3, replace=FALSE)
# split train data and test data
train <- data.matrix(iris[tr.idx,])
test <- data.matrix(iris[-tr.idx,])
# feature & Labels
train_feature <- train[,-5]
trainLabels <- train[,5]
test_feature <- test[,-5]
testLabels <- test[,5]
table(trainLabels)
# Build nn model
# first layers
my_input = mx.symbol.Variable('data')
fc1 = mx.symbol.FullyConnected(data=my_input, num.hidden = 200, name='fc1')
# Build nn model
# first layers
my_input = mx.symbol.Variable("iris")
# Build nn model
# first layers
my_input = mx.symbol.Variable('data')
# training
mx.set.seed(123)
device <- mx.cpu()
#install.packages('devtools')
library(devtools)
# Build nn model
# first layers
my_input = mx.symbol.Variable('data')
# lec16_2_cnn.r
# Convolutional Neural Network
# Require mxnet package
# install.packages("https://github.com/jeremiedb/mxnet_winbin/raw/master/mxnet.zip",repos = NULL)
library(mxnet)
# If you have Error message "no package called XML or DiagrmmeR", then install
#install.packages("XML")
#install.packages("DiagrammeR")
library(XML)
library(DiagrammeR)
# Load MNIST mn1
# 28*28, 1 channel images
mn1 <- read.csv("mini_mnist.csv")
set.seed(123)
N <- nrow(mn1)
tr.idx <- sample(1:N, size=N*2/3, replace=FALSE)
# split train data and test data
train_data <- data.matrix(mn1[tr.idx,])
test_data <- data.matrix(mn1[-tr.idx,])
test <- t(test_data[,-1]/255)
features <- t(train_data[,-1]/255)
labels <- train_data[,1]
# data preprocession
features_array <- features
dim(features_array) <- c(28,28,1,ncol(features))
test_array <- test
dim(test_array) <- c(28,28,1,ncol(test))
ncol(features)
table(labels)
# Build cnn model
# first conv layers
my_input = mx.symbol.Variable('data')
conv1 = mx.symbol.Convolution(data=my_input, kernel=c(4,4), stride=c(2,2), pad=c(1,1), num.filter = 20, name='conv1')
relu1 = mx.symbol.Activation(data=conv1, act.type='relu', name='relu1')
mp1 = mx.symbol.Pooling(data=relu1, kernel=c(2,2), stride=c(2,2), pool.type='max', name='pool1')
# second conv layers
conv2 = mx.symbol.Convolution(data=mp1, kernel=c(3,3), stride=c(2,2), pad=c(1,1), num.filter = 40, name='conv2')
relu2 = mx.symbol.Activation(data=conv2, act.type='relu', name='relu2')
mp2 = mx.symbol.Pooling(data=relu2, kernel=c(2,2), stride=c(2,2), pool.type='max', name='pool2')
# fully connected
fc1 = mx.symbol.FullyConnected(data=mp2, num.hidden = 1000, name='fc1')
relu3 = mx.symbol.Activation(data=fc1, act.type='relu', name='relu3')
fc2 = mx.symbol.FullyConnected(data=relu3, num.hidden = 3, name='fc2')
# softmax
sm = mx.symbol.SoftmaxOutput(data=fc2, name='sm')
# training
mx.set.seed(100)
device <- mx.cpu()
model <- mx.model.FeedForward.create(symbol=sm,
optimizer = "sgd",
array.batch.size=30,
num.round = 70, learning.rate=0.1,
X=features_array, y=labels, ctx=device,
eval.metric = mx.metric.accuracy,
epoch.end.callback=mx.callback.log.train.metric(100))
# test
predict_probs <- predict(model, test_array)
predicted_labels <- max.col(t(predict_probs)) - 1
table(test_data[, 1], predicted_labels)
sum(diag(table(test_data[, 1], predicted_labels)))/length(predicted_labels)
graph.viz(model$symbol)
# install packages
install.packages("xml2") # to read html
install.packages("rvest") # to use 'html_nodes'
install.packages("KoNLP") # korean natural language processing
install.packages("tm") # corpus, term-document matrix, etc.
install.packages("stringr") # to use 'str_match'
install.packages("wordcloud") # word cloud
# use packages
library(xml2)
library(rvest)
library(KoNLP)
library(tm)
library(stringr)
# crawling base URL
url_base <- 'http://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=51786&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page='
# start crawling
for(page in 1:10){
url <- paste(url_base, page, sep='') # from page 1 to 70
htxt <- read_html(url)
comment <- html_nodes(htxt, 'div')%>%html_nodes('div.input_netizen')%>%
html_nodes('div.score_result')%>%html_nodes('ul')%>%html_nodes('li')%>%
html_nodes('div.score_reple')%>%html_nodes('p') # exact location of comments
review <- html_text(comment) # extract only texts from comments
review <- repair_encoding(review, from = 'utf-8') # repair faulty encoding
review <- str_trim(review) # trim whitespace from start and end of string
reviews <- c(reviews, review) # save results
}
# make a vector to contain comments
reviews <- c()
# start crawling
for(page in 1:10){
url <- paste(url_base, page, sep='') # from page 1 to 70
htxt <- read_html(url)
comment <- html_nodes(htxt, 'div')%>%html_nodes('div.input_netizen')%>%
html_nodes('div.score_result')%>%html_nodes('ul')%>%html_nodes('li')%>%
html_nodes('div.score_reple')%>%html_nodes('p') # exact location of comments
review <- html_text(comment) # extract only texts from comments
review <- repair_encoding(review, from = 'utf-8') # repair faulty encoding
review <- str_trim(review) # trim whitespace from start and end of string
reviews <- c(reviews, review) # save results
}
# extract nouns(N) and predicates(P)
ext_func <- function(doc){
doc_char <- as.character(doc)
ext1 <- paste(SimplePos09(doc_char))
ext2 <- str_match(ext1, '([A-Z가-힣]+)/[NP]')
keyword <- ext2[,2]
keyword[!is.na(keyword)]
}
# 1. term-document matrix
corp <- Corpus(VectorSource(reviews)) # generate a corpus
tdm <- TermDocumentMatrix(corp, # generate a term-document matrix
control=list(
tokenize=ext_func,
removePunctuation=T,
removeNumbers=T,
wordLengths=c(4,8)))
tdm_matrix <- as.matrix(tdm) # save as a matrix
Encoding(rownames(tdm_matrix)) <- "UTF-8" # encoding
word_count <- rowSums(tdm_matrix) # sum of term frequencies of each word
word_order <- word_count[order(word_count, decreasing=T)] # sort in descending order
doc <- as.data.frame(word_order) # save as a data frame
windowsFonts(font=windowsFont("맑은 고딕")) # set font
set.seed(1234)
wordcloud(words=rownames(doc), freq=doc$word_order, min.freq=2,
max.words=100, random.order=FALSE, scale=c(5,1), rot.per=0.35,
family="font", colors=brewer.pal(8,"Dark2"))
# 2. word cloud
library(wordcloud)
windowsFonts(font=windowsFont("맑은 고딕")) # set font
set.seed(1234)
wordcloud(words=rownames(doc), freq=doc$word_order, min.freq=2,
max.words=100, random.order=FALSE, scale=c(5,1), rot.per=0.35,
family="font", colors=brewer.pal(8,"Dark2"))
